{
    "cells": [
        {
            "kind": "markdown",
            "source": "## What is KQL?  \r\nKusto Query Language (KQL) is a powerful, read-only query language designed to efficiently analyze large volumes of data. Originally created for Azure Data Explorer, it is now widely used across Microsoft services — including Microsoft Fabric, specifically for real-time analytics scenarios.  \r\n\r\n## Key Features  \r\n\r\n**Optimized for log and telemetry data:**  \r\nKQL can work with data from structured, semi-structured, and unstructured sources — as long as it is ingested into strongly typed tables.  \r\n\r\n**Powerful data exploration:**  \r\nSupports rich querying capabilities like filtering, aggregation, joins, time series analysis, and advanced functions.  \r\n\r\n**Intuitive syntax:**  \r\nSimilar to SQL in some aspects, but focused on data exploration and transformation rather than data manipulation (e.g., no INSERT, UPDATE, or DELETE).  \r\n\r\n**Extensive function library:**  \r\nIncludes built-in operators and functions for analytics, machine learning plugins, and visualization.  \r\n\r\n## Core Concepts  \r\n\r\n**Strongly typed tables:**  \r\nAll data, regardless of its original form, is organized into tables with explicitly defined schemas and strongly typed columns after ingestion. Flexible fields can be stored in dynamic columns to handle semi-structured data like JSON.  \r\n\r\n**Operators and commands:**  \r\nQueries are constructed using operators connected by the pipe (`|`) character, enabling clear, step-by-step data transformation.  \r\n\r\n**Schemas and metadata:**  \r\nExplicit schemas support query optimization and advanced analytics.  \r\n\r\n## Typical Usage  \r\n\r\n- Log analytics and monitoring  \r\n- Interactive data exploration  \r\n- Real-time dashboards and anomaly detection  \r\n\r\n## Where to use KQL in Microsoft Fabric?  \r\n\r\n**Real-Time Analytics workloads:**  \r\nNative and fully supported (KQL is the primary language). Fabric uses the Eventhouse storage layer to ingest and analyze large volumes of streaming and log data using KQL.  \r\n\r\n**Data Warehouses and Lakehouses:**  \r\nThese do not directly support KQL. However, you can ingest or reference data from these systems into Real-Time Analytics environments and then query it using KQL.  \r\n\r\n## Query Statements in KQL\r\n\r\n### Types of Query Statements  \r\n\r\nThere are **three types of user query statements**:  \r\n- **Tabular expression statements** — the most common; both input and output are tables.  \r\n- **Let statements** — define variables or expressions for reuse.  \r\n- **Set statements** — set query-scoped options or parameters.  \r\n\r\n### Tabular Expression Statements  \r\n\r\nThe main statement type for querying data.  \r\nComposed of **operators**, connected by pipes (`|`), each taking a table as input and returning a table as output.  \r\nActs like a **funnel**: each operator filters, rearranges, or summarizes data progressively.  \r\n**Operator order matters**, as it affects both the final result and query performance.  \r\n\r\n**Example Query:**  \r\n\r\n```kusto\r\nStormEvents\r\n| where StartTime between (datetime(2007-11-01) .. datetime(2007-12-01))\r\n| where State == \"FLORIDA\"\r\n| count;\r\n```\r\n\r\n(1) Starts with the `StormEvents` table.  \r\n(2) Filters by `StartTime` and `State`.  \r\n(3) Finally counts the remaining rows (result: 28).  \r\n\r\n### Case Sensitivity  \r\n\r\n**KQL is case-sensitive** for table names, column names, operators, and functions.  \r\nKeywords can be used as identifiers when enclosed in brackets (e.g., `['where']`).  \r\n\r\n### Management Commands  \r\n\r\nUnlike queries, **management commands** modify or manage data or metadata (e.g., create tables).  \r\nSyntax starts with a dot (`.`), preventing them from being embedded in queries.  \r\n\r\n**Example:**  \r\n\r\n```kusto\r\n.create table Logs (Level:string, Text:string)\r\n```\r\n\r\nCommands like `.show tables` return metadata (e.g., list of all tables).  \r\n\r\n# KQL quick reference\r\n\r\nA list of functions and their descriptions to help get you started using Kusto Query Language.\r\n\r\n## Filter/Search/Condition\r\n\r\n| Operator/Function       | Description                                                                                                   | Syntax                                                         |\r\n| :---------------------- | :------------------------------------------------------------------------------------------------------------ |:---------------------------------------------------------------|\r\n| where                   | Filters on a specific predicate                                                                               | `T \\| where Predicate`                                         |\r\n| where contains/has     | `Contains`: Looks for any substring match <br> `Has`: Looks for a specific word (better performance)          | `T \\| where col1 contains/has \"[search term]\"`                |\r\n| search                  | Searches all columns in the table for the value                                                               | `[TabularSource \\|] search [kind=CaseSensitivity] [in (TableSources)] SearchPredicate` |\r\n| take                    | Returns the specified number of records. Use to test a query.<br>**_Note_**: `take` and `limit` are synonyms. | `T \\| take NumberOfRows`                                      |\r\n| case                    | Adds a condition statement, similar to if/then/elseif in other systems.                                      | `case(predicate_1, then_1, predicate_2, then_2, predicate_3, then_3, else)` |\r\n| distinct                | Produces a table with the distinct combination of the provided columns of the input table                      | `distinct [ColumnName], [ColumnName]`                         |\r\n\r\n<br>\r\n  \r\n## Date/Time\r\n\r\n| Operator/Function | Description                                                                                                         | Syntax                              |\r\n| :---------------- | :------------------------------------------------------------------------------------------------------------------ |:------------------------------------|\r\n| ago               | Returns the time offset relative to the time the query executes. For example, `ago(1h)` is one hour before the current clock's reading. | `ago(a_timespan)`                  |\r\n| format_datetime   | Returns data in various date formats.                                                                              | `format_datetime(datetime , format)` |\r\n| bin              | Rounds all values in a timeframe and groups them                                                                  | `bin(value, roundTo)`              |\r\n\r\n<br>\r\n\r\n## Create/Remove Columns\r\n  \r\n| Operator/Function   | Description                                                      | Syntax                                         |\r\n| :------------------ | :--------------------------------------------------------------- |:-----------------------------------------------|\r\n| print               | Outputs a single row with one or more scalar expressions         | `print [ColumnName =] ScalarExpression [, ...]` |\r\n| project             | Selects the columns to include in the order specified            | `T \\| project ColumnName [= Expression] [, ...]` |\r\n| project-away        | Selects the columns to exclude from the output                   | `T \\| project-away ColumnNameOrPattern [, ...]` |\r\n| project-keep        | Selects the columns to keep in the output                        | `T \\| project-keep ColumnNameOrPattern [, ...]` |\r\n| project-rename      | Renames columns in the result output                             | `T \\| project-rename new_column_name = column_name` |\r\n| project-reorder     | Reorders columns in the result output                            | `T \\| project-reorder Col2, Col1, Col* asc`   |\r\n| extend              | Creates a calculated column and adds it to the result set       | `T \\| extend [ColumnName =] Expression [, ...]` |\r\n\r\n<br>\r\n\r\n## Sort and Aggregate Dataset\r\n  \r\n| Operator/Function | Description                                                                                               | Syntax                                                                                  |\r\n| :---------------- | :-------------------------------------------------------------------------------------------------------- |:----------------------------------------------------------------------------------------|\r\n| sort operator     | Sort the rows of the input table by one or more columns in ascending or descending order                  | `T \\| sort by expression1 [asc\\|desc], expression2 [asc\\|desc], …`                     |\r\n| top               | Returns the first N rows of the dataset when the dataset is sorted using `by`                             | `T \\| top numberOfRows by expression [asc\\|desc] [nulls first\\|last]`                 |\r\n| summarize         | Groups the rows according to the `by` group columns, and calculates aggregations over each group          | `T \\| summarize [[Column =] Aggregation [, ...]] [by [Column =] GroupExpression [, ...]]` |\r\n| count             | Counts records in the input table (for example, T).<br>This operator is shorthand for `summarize count()`. | `T \\| count`                                                                           |\r\n| join              | Merges the rows of two tables to form a new table by matching values of the specified column(s) from each table. Supports a full range of join types: `fullouter`, `inner`, `innerunique`, `leftanti`, `leftantisemi`, `leftouter`, `leftsemi`, `rightanti`, `rightantisemi`, `rightouter`, `rightsemi`. | `LeftTable \\| join [JoinParameters] (RightTable) on Attributes`                        |\r\n| union             | Takes two or more tables and returns all their rows                                                       | `[T1] \\| union [T2], [T3], …`                                                          |\r\n| range             | Generates a table with an arithmetic series of values                                                     | `range columnName from start to stop step step`                                        |\r\n\r\n<br>\r\n\r\n## Format Data\r\n\r\n| Operator/Function | Description                                                                                                                       | Syntax                                                                                              |\r\n| :---------------- | :-------------------------------------------------------------------------------------------------------------------------------- |:----------------------------------------------------------------------------------------------------|\r\n| lookup            | Extends the columns of a fact table with values looked-up in a dimension table                                                    | `T1 \\| lookup [kind = (leftouter\\|inner)] (T2) on Attributes`                                     |\r\n| mv-expand         | Turns dynamic arrays into rows (multi-value expansion)                                                                            | `T \\| mv-expand Column`                                                                            |\r\n| parse             | Evaluates a string expression and parses its value into one or more calculated columns. Use for structuring unstructured data.    | `T \\| parse [kind=regex [flags=regex_flags]\\|simple\\|relaxed] Expression with * (StringConstant ColumnName [: ColumnType]) *...` |\r\n| make-series       | Creates series of specified aggregated values along a specified axis                                                              | `T \\| make-series [MakeSeriesParameters] [Column =] Aggregation [default = DefaultValue] [, ...] on AxisColumn from start to end step step [by [Column =] GroupExpression [, ...]]` |\r\n| let               | Binds a name to expressions that can refer to its bound value. Values can be lambda expressions to create query-defined functions as part of the query. Use `let` to create expressions over tables whose results look like a new table. | `let Name = ScalarExpression \\| TabularExpression \\| FunctionDefinitionExpression`                |\r\n\r\n<br>\r\n  \r\n## General\r\n\r\n| Operator/Function | Description                                        | Syntax                                               |\r\n| :---------------- | :------------------------------------------------- |:-----------------------------------------------------|\r\n| invoke            | Runs the function on the table that it receives as input. | `T \\| invoke function([param1, param2])`            |\r\n| evaluate pluginName | Evaluates query language extensions (plugins).    | `[T \\|] evaluate [evaluateParameters] PluginName (PluginArgs...)` |\r\n\r\n<br>\r\n  \r\n## Visualization\r\n\r\n| Operator/Function | Description                          | Syntax                                                           |\r\n| :---------------- | :----------------------------------- |:-----------------------------------------------------------------|\r\n| render            | Renders results as a graphical output | `T \\| render Visualization [with (PropertyName = PropertyValue [, ...])]` |",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `where` Operator\r\n\r\n### What it does  \r\n\r\nThe `where` operator **filters rows from a tabular dataset** based on a Boolean predicate expression.  \r\nSimilar to the SQL `WHERE` clause, but used as an operator in a query pipeline.  \r\n\r\n### Syntax  \r\n\r\n```kusto\r\nT | where Predicate\r\n```  \r\n\r\n- `T`: input table or tabular expression.  \r\n- `Predicate`: condition expression returning `true` or `false`.  \r\n\r\n### Performance considerations  \r\n\r\n- **Predicate pushdown:**  \r\n  Applied as early as possible to reduce row count before subsequent operators → major performance gain.  \r\n\r\n- **Index and partition pruning:**  \r\n  Conditions on indexed columns (e.g., `ingestion_time()` or time filters) allow skipping partitions.  \r\n\r\n- **Filter selectivity:**  \r\n  The more restrictive (selective) a condition is, the better the performance benefit, since it reduces data volume for subsequent operators.  \r\n\r\n- **Prefer simple column-to-constant comparisons:**  \r\n  Use conditions like `where Timestamp >= ago(1d)` instead of more complex computed expressions such as `where bin(Timestamp, 1d) == ago(1d)`.  \r\n  → Simple comparisons allow better partition pruning and query plan optimizations.  \r\n\r\n- **Order of `and` clauses matters:**  \r\n  When using multiple conditions combined with `and`, place the **most selective and simplest conditions first**.  \r\n  Example:  \r\n  ```kusto\r\n  where Timestamp > ago(1d) and OpId == EventId\r\n  ```  \r\n  Here, the time-based filter is typically more selective and reduces the dataset significantly before the next condition is evaluated.  \r\n  ⚠️ This order should be chosen **based on actual data selectivity**, not rigidly.  \r\n\r\n- **Constants in predicates:**  \r\n  Constants can include functions like `now()` or `ago()` as well as values defined with `let`.  \r\n  ⚠️ now() is evaluated once per query, not per row, and can be treated as a constant in predicates. Using let is optional but improves readability and maintainability when reusing the value.\r\n\r\n### Expression support  \r\n\r\nSupports comparisons (`==`, `!=`, `<`, `>`, etc.), logical operators (`and`, `or`, `not`), scalar functions, and calculated expressions.  \r\nSupports case transformations with `tolower()` or `toupper()`.  \r\n\r\n### Handling NULL values  \r\n\r\nMissing or undefined values are represented as `null`.  \r\nAny comparison involving `null` evaluates to `false`.  \r\nTo explicitly check for `null`, use:  \r\n```kusto\r\nwhere isnull(ColumnName)\r\nwhere isnotnull(ColumnName)\r\n```  \r\n\r\n### Multiple `where` operators  \r\n\r\nCan be chained for incremental filtering.  \r\nCombining conditions into a single `where` is possible, but chaining can improve readability.  \r\n\r\n### Example  \r\n\r\n```kusto\r\nStormEvents\r\n| where StartTime >= datetime(2007-11-01) and StartTime < datetime(2007-12-01)\r\n| where State == \"FLORIDA\"\r\n| where isnotnull(EventType)\r\n| count\r\n```  \r\n\r\n### Key takeaway  \r\n\r\n> The `where` operator is critical for **performance optimization**: it reduces data volume early (predicate pushdown), enables partition pruning, and handles `null` values explicitly. Writing selective, simple predicates first in `and` clauses maximizes efficiency, but the actual order should reflect the most selective filters based on the data. By default, tables in Kusto are partitioned according to the time at which data is ingested. In the majority of use cases.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where State is \"FLORIDA\" and EventType is \"Flood\", \"Hurricane\", or \"Tornado\",\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where State == \"FLORIDA\" and EventType in (\"Flood\", \"Hurricane\", \"Tornado\")\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where State is \"FLORIDA\" and EventType is \"FLOOD\", \"HURRICANE\", or \"TORNADO\",\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where State == \"FLORIDA\" and EventType in (\"FLOOD\", \"HURRICANE\", \"TORNADO\")\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where State is \"FLORIDA\" and EventType matches (case-insensitive) \"FLOOD\", \"HURRICANE\", or \"TORNADO\",\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where State == \"FLORIDA\" and EventType in~ (\"FLOOD\", \"HURRICANE\", \"TORNADO\")\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where the 'EpisodeNarrative' column contains the word \"MAJOR\" (case-insensitive),\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where EpisodeNarrative has \"MAJOR\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where any column contains the word \"MAJOR\" (case-insensitive),\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where * has \"MAJOR\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where any column contains the word \"MAJOR\" (case-sensitive),\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where * has_cs \"MAJOR\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where 'EpisodeNarrative' contains the string \"maj\" (case-insensitive),\r\n// and returns up to 10 results.\r\n// Note: 'contains' is case-insensitive and checks for substring matches, \r\n// while 'has' looks for whole terms (token-based).\r\ndatabase('Samples').StormEvents\r\n| where EpisodeNarrative contains \"maj\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where 'EpisodeNarrative' does not contain the string \"maj\" (case-sensitive),\r\n// and returns up to 10 results.\r\n// Note: 'contains' is case-insensitive and checks for substring matches, \r\n// while 'has' looks for whole terms (token-based).\r\ndatabase('Samples').StormEvents\r\n| where EpisodeNarrative !contains_cs \"maj\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where 'EventType' starts with \"ThunderStorm\" (case-insensitive),\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where EventType startswith \"ThunderStorm\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// filters for rows where 'EventType' ends with \"Wind\" (case-sensitive),\r\n// and returns up to 10 results.\r\ndatabase('Samples').StormEvents\r\n| where EventType endswith_cs \"Wind\"\r\n| limit 10",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `search` operator in KQL\r\n\r\n### Purpose\r\nThe `search` operator is used to **search across multiple tables at once**, or across **all columns in a table**, for one or more text patterns or keywords.  \r\nIt is intended for **free-text or exploratory searches**, similar to full-text search in log data.  \r\n\r\n### Behavior\r\nBy default, `search` looks at **all columns**, but it can be restricted to specific columns.  \r\nIt is **case-insensitive** unless explicitly using case-sensitive variants (e.g., `has_cs`).  \r\n\r\n### Difference from `where`\r\n`where` is more efficient when filtering based on specific, well-defined conditions on certain columns.  \r\n`search` is more flexible for broad, exploratory searches across many fields.  \r\n\r\n### Restricting to tables\r\nYou can limit `search` to certain tables using `in`, for example:  \r\n\r\n```kusto\r\nsearch in (Table1, Table2) \"error\"\r\n```  \r\n\r\n### Restricting to columns\r\nYou can also specify which columns to search in using `in`.  \r\n\r\n### Remarks\r\nUnlike the `find` operator, the `search` operator does **not** support the following syntax:  \r\n\r\n- `withsource=:` — The output always includes a column called `$table` (type string), which contains the name of the table each record came from (or a system-generated name if not from a table).  \r\n- `project=`, `project-smart` — The `search` operator does not support these options for customizing output columns.  \r\n  Instead, it automatically selects a relevant set of columns, equivalent to what `project-smart` would select in the `find` operator.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// searches all columns for the term \"flood\" (case-insensitive),\r\n// and returns up to 50 results.\r\ndatabase('Samples').StormEvents\r\n| search \"flood\"\r\n| take 50",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// searches all columns for the exact (case-sensitive) term \"FLOOD\",\r\n// and returns up to 50 results.\r\ndatabase('Samples').StormEvents\r\n| search kind=case_sensitive \"FLOOD\"\r\n| take 50",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Searches all tables and all columns in the current database\r\n// for the term \"test\" (case-insensitive),\r\n// and returns up to 300 results.\r\nsearch \"test\"\r\n| take 300",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Searches the 'StormEvents' table in the 'Samples' database and the 'SecurityLogs' table in the 'SampleLogs' database,\r\n// looks for the term \"test\" (case-insensitive) across all columns,\r\n// and returns up to 50 results.\r\nsearch in (database('Samples').StormEvents, database('SampleLogs').SecurityLogs) \"test\"\r\n| take 50",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// searches all columns for values that start with \"ThunderStorm\" (case-insensitive),\r\n// and returns up to 50 results.\r\ndatabase('Samples').StormEvents\r\n| search * startswith (\"ThunderStorm\")\r\n| take 50",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// searches all columns for values that end with \"Wind\" (case-insensitive),\r\n// and returns up to 50 results.\r\ndatabase('Samples').StormEvents\r\n| search * endswith (\"Wind\")\r\n| take 50",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// searches all columns for values matching the wildcard pattern \"W*nd\" (case-insensitive),\r\n// and returns up to 50 results.\r\ndatabase('Samples').StormEvents\r\n| search (\"W*nd\")\r\n| take 50",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `isnull()` function in KQL  \r\n\r\n### Purpose  \r\n\r\nThe `isnull()` function is used to **check if a scalar expression (typically a column value) is null**.  \r\nIt returns a boolean value: `true` if the value is null, and `false` otherwise.  \r\n\r\n### Behavior\r\n\r\n* Evaluates each row and returns `true` for rows where the specified column or expression is actually `null`.  \r\n* Commonly used in `where` clauses to filter out or isolate null values, or in `extend` to flag missing data.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nisnull(<expression>)\r\n```\r\n\r\n### Performance notes  \r\n\r\n* Evaluated per row and very fast, since it performs only a null-check.  \r\n* Can be combined with functions like `coalesce()`, `iif()`, or used within conditional projections.  \r\n\r\n### Important String-Type Caveat\r\n\r\n* **String-typed columns in KQL are never truly null.**  \r\n  Any column defined as `string` will always contain at least `\"\"` (an empty string), never `null`.  \r\n* As a result, calling `isnull()` on a `string` column **always returns `false`**.  \r\n* To detect empty or missing text values, use `isempty()` (or `isblank()`, if you need to treat whitespace-only strings as blank).  \r\n\r\n### Remarks\r\n\r\n* Works as expected on numeric, datetime, `dynamic`, and other nullable types.  \r\n* For `dynamic` columns, `isnull()` will return `true` if the value is `dynamic(null)`.  \r\n* Use `isnull()` for types that can genuinely be null; for string columns, prefer `isempty()`.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves storm event records from the 'StormEvents' table in the 'Samples' database,\r\n// specifically looking for events where both 'BeginLat' (latitude) and 'BeginLon' (longitude) are null.\r\n//\r\n// It uses the 'isnull()' function to check for null numeric values in both columns.\r\n// Note: The isnull function is used specifically for numeric values.\r\n// Only rows where both latitude and longitude are null are included.\r\n//\r\n// Then, it projects (selects) specific columns to include in the result:\r\n//   - 'BeginLat'\r\n//   - 'BeginLon'\r\n//   - 'StartTime'\r\n//   - 'EndTime'\r\n//   - 'EpisodeId'\r\n//   - 'EventId'\r\n//   - 'State'\r\n//   - 'EventType'\r\n//\r\n// Finally, it limits the output to the first 10 matching rows.\r\ndatabase('Samples').StormEvents\r\n| where isnull(BeginLat) and isnull(BeginLon)\r\n| project BeginLat, BeginLon, StartTime, EndTime, EpisodeId, EventId, State, EventType\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves storm event records from the 'StormEvents' table in the 'Samples' database,\r\n// filtering for events where the 'State' column is null and projecting key event fields.\r\n// The selected columns are BeginLat, BeginLon, StartTime, EndTime, EpisodeId, EventId, State, and EventType,\r\n// then the result is limited to 10 rows using 'take'.\r\n//\r\n// It uses the 'isnull()' function to check for null values in the string 'State' column.\r\n// Note: In KQL, string columns can never actually be null, so isnull(State) always returns false,\r\n// meaning no records match and the query returns no rows despite the 'take 10' operator.\r\ndatabase('Samples').StormEvents\r\n| where isnull( State) \r\n| project BeginLat, BeginLon, StartTime, EndTime, EpisodeId, EventId, State, EventType\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `isnotnull()` function in KQL\r\n\r\n### Purpose\r\n\r\nThe `isnotnull()` function is used to **check if a scalar expression (typically a column value) is not null**.  \r\nIt returns a boolean value: `true` if the value is not null, and `false` if it is null.  \r\n\r\n### Behavior\r\n\r\n* Evaluates each row and returns `true` for rows where the specified column or expression contains a value (i.e., is not `null`).  \r\n* Commonly used in `where` clauses to include only rows with valid (non-null) data, or in `extend` to define conditional flags or categories.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nisnotnull(<expression>)\r\n```\r\n\r\n### Performance notes\r\n\r\n* Evaluated per row and highly efficient, as it only checks for the presence of a non-null value.  \r\n* Can be combined with functions such as `iif()`, `coalesce()`, or used directly in conditional logic.  \r\n\r\n### Important String-Type Caveat\r\n\r\n* **String-typed columns in KQL are never truly null.**  \r\n  Any column defined as `string` will always contain at least `\"\"` (an empty string), never `null`.  \r\n* Therefore, calling `isnotnull()` on a `string` column **always returns `true`**.  \r\n* To detect non-empty text values, use `isnotempty()` (or `isnotblank()`, if you need to treat whitespace-only strings as blank).  \r\n\r\n### Remarks\r\n\r\n* Works as expected on numeric, datetime, `dynamic`, and other nullable types.  \r\n* For `dynamic` columns, `isnotnull()` returns `false` only if the value is `dynamic(null)`.  \r\n* Use `isnotnull()` for types that can genuinely be null; for string columns, prefer `isnotempty()`/`isnotblank()`.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'StormEvents' table in the 'Samples' database,\r\n// specifically selecting rows where both 'BeginLat' (latitude) and 'BeginLon' (longitude) are not null.\r\n//\r\n// It uses the 'isnotnull()' function to ensure that both coordinates have valid (non-null) values.\r\n//\r\n// Then, it projects (selects) relevant columns to include in the output:\r\n//   - 'BeginLat'\r\n//   - 'BeginLon'\r\n//   - 'StartTime'\r\n//   - 'EndTime'\r\n//   - 'EpisodeId'\r\n//   - 'EventId'\r\n//   - 'State'\r\n//   - 'EventType'\r\n//\r\n// Finally, it limits the result to the first 10 matching rows.\r\n//\r\n// This helps identify events with complete coordinate information.\r\ndatabase('Samples').StormEvents\r\n| where isnotnull(BeginLat) and isnotnull(BeginLon)\r\n| project BeginLat, BeginLon, StartTime, EndTime, EpisodeId, EventId, State, EventType\r\n| take 10",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `coalesce()` function in KQL\r\n\r\n### Purpose\r\nThe `coalesce()` function is used to **return the first non-null value from a list of expressions**, similar to the SQL `COALESCE` function.  \r\nIt is commonly used to replace nulls with fallback values or to combine multiple possible sources into a single output column.  \r\n\r\n### Behavior\r\n`coalesce()` evaluates each argument in order and returns the first one that is not null.  \r\nIf all arguments are null, the result is null.  \r\n\r\n### Syntax\r\n\r\n```kql\r\ncoalesce(expression1, expression2, ..., expressionN)\r\n```  \r\n\r\n### Performance notes\r\n`coalesce()` is evaluated row by row and stops at the first non-null value, making it efficient even when multiple arguments are provided.  \r\nIt avoids evaluating additional expressions once a valid value is found, which can save computation time.  \r\n\r\n### Remarks\r\n- Can accept any number of arguments.  \r\n- Arguments should be of the same or compatible types to avoid type conversion errors.  \r\n- Especially useful in `extend`, `project`, or `summarize` to provide default values for missing data.  \r\n- Enhances query readability and robustness by handling null values gracefully.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'DailyCovid19' table in the 'Samples' database.\r\n//\r\n// It uses 'extend' to add or modify the 'CountryCode' column:\r\n// - The 'coalesce()' function checks if 'CountryCode' is null or empty.\r\n// - If it is null, it replaces it with the string \"Not Assigned\".\r\n// - Otherwise, the original value of 'CountryCode' is kept.\r\n//\r\n// Finally, it limits the result to the first 10 rows.\r\n//\r\n// This ensures that any rows without a specified country code will explicitly display \"Not Assigned\" instead of being left blank.\r\ndatabase('Samples').DailyCovid19\r\n| extend CountryCode = coalesce(CountryCode, \"Not Assigned\")\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'StormEvents' table in the 'Samples' database,\r\n// specifically selecting rows where both 'BeginLat' (latitude) and 'BeginLon' (longitude) are null.\r\n//\r\n// The query uses the 'isnull()' function to filter rows with null numeric values.\r\n//\r\n// Then, it uses 'extend' to transform these columns:\r\n// - Each 'BeginLat' and 'BeginLon' value is first cast to string using 'tostring()', since they are originally numeric.\r\n// - The 'coalesce()' function is then applied to replace null string values with \"Not Assigned\".\r\n//   This ensures that when these columns are displayed, they show \"Not Assigned\" instead of remaining blank.\r\n//\r\n// It then projects (selects) relevant columns to include in the output:\r\n//   - 'BeginLat' (now replaced if null)\r\n//   - 'BeginLon' (now replaced if null)\r\n//   - 'StartTime'\r\n//   - 'EndTime'\r\n//   - 'EpisodeId'\r\n//   - 'EventId'\r\n//   - 'State'\r\n//   - 'EventType'\r\n//\r\n// Finally, it limits the result to the first 10 matching rows.\r\ndatabase('Samples').StormEvents\r\n| where isnull(BeginLat) and isnull(BeginLon)\r\n| extend\r\n    BeginLat = coalesce(tostring(BeginLat), \"Not Assigned\"),\r\n    BeginLon = coalesce(tostring(BeginLon), \"Not Assigned\")\r\n| project BeginLat, BeginLon, StartTime, EndTime, EpisodeId, EventId, State, EventType\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `isempty()` function in KQL\r\n\r\n### Purpose\r\nThe `isempty()` function is used to **check whether a string expression is empty or null**.  \r\nIt returns a boolean value: `true` if the string is either an empty string (\"\") or null, and `false` otherwise.  \r\n\r\n### Behavior\r\n`isempty()` evaluates each row and treats both null values and empty strings as \"empty\".  \r\nThis makes it especially useful when working with user-entered data or optional text fields where missing or blank values should be handled consistently.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nisempty(expression)\r\n```  \r\n\r\n### Performance notes\r\n`isempty()` is evaluated per row and is highly efficient since it only checks for string emptiness or nullity.  \r\nIt can be combined with functions such as `iif()`, `coalesce()`, or used directly in `where` clauses to filter rows with missing text values.  \r\n\r\n### Remarks\r\n- Designed specifically for string columns; it is not applicable to numeric or datetime columns.  \r\n- Commonly used for cleaning data, flagging incomplete records, or providing default text values.  \r\n- Supports improving query readability by clearly expressing checks for missing or blank strings.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'DailyCovid19' table in the 'Samples' database,\r\n// specifically selecting rows where the 'CountryCode' field is empty.\r\n//\r\n// It uses the 'isempty()' function, which checks if a string column is either null or an empty string (\"\").\r\n// This function is designed specifically for string values.\r\n//\r\n// Finally, it limits the result to the first 10 matching rows.\r\n//\r\n// This helps identify records missing a country code, which may need to be corrected or flagged in further analysis.\r\ndatabase('Samples').DailyCovid19\r\n| where isempty(CountryCode)\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'DailyCovid19' table in the 'Samples' database.\r\n//\r\n// It uses 'extend' to add or update the 'CountryCode' column:\r\n// - The 'iif()' function evaluates the condition 'isempty(CountryCode)'.\r\n// - If 'CountryCode' is empty (null or empty string), it assigns the value \"Not Assigned\".\r\n// - Otherwise, it keeps the original 'CountryCode' value.\r\n//\r\n// Finally, it limits the result to the first 10 rows.\r\n//\r\n// This ensures that any rows with an empty country code are explicitly labeled \"Not Assigned\" instead of showing blank values.\r\ndatabase('Samples').DailyCovid19\r\n| extend CountryCode = iif(isempty(CountryCode), \"Not Assigned\", CountryCode)\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'StormEvents' table in the 'Samples' database,\r\n// specifically selecting rows where both 'BeginLat' (latitude) and 'BeginLon' (longitude) are null.\r\n//\r\n// It uses 'isnull()' to filter rows with null numeric coordinate values.\r\n//\r\n// Then, it uses 'extend' to transform these columns:\r\n// - First, it casts each numeric column ('BeginLat' and 'BeginLon') to string using 'tostring()'.\r\n// - The 'isempty()' function is then used to check if the converted string is empty (which would be the case for null values).\r\n// - The 'iff()' function replaces empty values with \"Not Assigned\"; otherwise, it keeps the original stringified value.\r\n//\r\n// This involves two conversions:\r\n//   - The first conversion makes it possible to apply 'isempty()', since it works on strings.\r\n//   - The second ensures that non-null numeric values are consistently treated as strings.\r\n//\r\n// Finally, it projects (selects) the transformed columns along with other relevant event details,\r\n// and limits the output to the first 10 matching rows.\r\ndatabase('Samples').StormEvents\r\n| where isnull(BeginLat) and isnull(BeginLon)\r\n| extend\r\n    BeginLat = iff(isempty(tostring(BeginLat)), \"Not Assigned\", tostring(BeginLat)),\r\n    BeginLon = iff(isempty(tostring(BeginLon)), \"Not Assigned\", tostring(BeginLon))\r\n| project BeginLat, BeginLon, StartTime, EndTime, EpisodeId, EventId, State, EventType\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `isnotempty()` function in KQL\r\n\r\n### Purpose\r\nThe `isnotempty()` function is used to **check whether a string expression is not empty and not null**.  \r\nIt returns a boolean value: `true` if the string contains any non-empty value, and `false` if it is null or an empty string (\"\").  \r\n\r\n### Behavior\r\n`isnotempty()` evaluates each row and returns `true` for strings that are neither null nor empty.  \r\nIt is particularly useful for ensuring that text fields contain meaningful data before further processing or analysis.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nisnotempty(expression)\r\n```  \r\n\r\n### Performance notes\r\n`isnotempty()` is evaluated per row and is highly efficient, as it performs a simple string content check.  \r\nIt is commonly used in `where` clauses to exclude incomplete or missing string values and in `extend` to build flags or derived columns.  \r\n\r\n### Remarks\r\n- Designed specifically for string columns; it does not apply to numeric or datetime columns.  \r\n- Helps to explicitly include only rows with valid text data.  \r\n- Improves readability and intent when filtering for populated fields.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves records from the 'DailyCovid19' table in the 'Samples' database,\r\n// specifically selecting rows where the 'CountryCode' field is not empty.\r\n//\r\n// It uses the 'isnotempty()' function, which checks if a string column is neither null nor an empty string (\"\").\r\n//\r\n// Finally, it limits the result to the first 10 matching rows.\r\n//\r\n// This helps focus on records where a valid country code is present and excludes any incomplete or missing entries.\r\ndatabase('Samples').DailyCovid19\r\n| where isnotempty(CountryCode)\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `take` operator in KQL\r\n\r\n### Purpose\r\nThe `take` operator **returns up to N rows** from the input, chosen arbitrarily.  \r\nIt is functionally equivalent to `limit`, which is simply an alias.  \r\n\r\n### Usage\r\nCommonly used to **quickly preview data** without processing the full dataset.  \r\nSyntax example:  \r\n```kql\r\nTableName\r\n| take 10\r\n```  \r\n\r\n### Behavior\r\nThe rows returned by `take` are **not ordered** — they are chosen arbitrarily from the input set.  \r\nTo return a specific set of rows in a known order, use `sort by` first, for example:  \r\n```kql\r\nTableName\r\n| sort by Timestamp desc\r\n| take 5\r\n```  \r\nWorks efficiently even on large datasets since it does not require scanning or sorting all rows unless combined with sorting.  \r\n\r\n### Equivalence\r\n`take` and `limit` can be used interchangeably:  \r\n```kql\r\nTableName\r\n| limit 20\r\n```  \r\ndoes the same as  \r\n```kql\r\nTableName\r\n| take 20\r\n```  ",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `ago()` function in KQL\r\n\r\n### Purpose\r\nThe `ago()` function is used to **calculate a datetime value relative to the current time (now)**.  \r\nIt simplifies filtering or comparing timestamps to a time window in the past (e.g., \"events from the last 7 days\").  \r\n\r\n### Behavior\r\nWhen called, `ago(timespan)` returns the datetime value representing \"now minus timespan\".  \r\nThe `now()` function is evaluated at the time the query is run, so `ago()` always dynamically reflects the current time.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nago(timespan)\r\n```  \r\nFor example, `ago(1d)` returns the datetime value for \"one day ago from now\".  \r\n\r\n### Difference from using `datetime arithmetic`\r\n- `ago()` is a convenient shorthand for `now() - timespan`.\r\n- Improves readability and conciseness in filters.  \r\n\r\n### Performance notes\r\n`ago()` is computed once per query execution and can be reused within the same query.  \r\nIt does not add overhead but can help the query engine optimize time-based filters.  \r\n\r\n### Remarks\r\n- Commonly used in `where` clauses to filter recent data, for example: `where Timestamp > ago(7d)`.  \r\n- Supports any valid timespan (e.g., seconds, minutes, hours, days).  \r\n- You can assign `ago()` to a variable using `let` to improve readability and reuse in complex queries.  \r\n\r\n### Common examples\r\n- `ago(7d)` → 7 days ago  \r\n- `ago(1h)` → 1 hour ago  \r\n- `ago(30m)` → 30 minutes ago  \r\n\r\n### Benefits\r\nUsing `ago()` makes time filters more **maintainable and readable** compared to hardcoded datetime values.  \r\nIt also avoids the need to manually update queries with static timestamps.  \r\n\r\n## `now()` function in KQL\r\n\r\n### Purpose\r\nThe `now()` function is used to **return the current UTC datetime value** at the moment the query starts running.  \r\nIt is commonly used for calculating relative time windows or generating current timestamps in queries.  \r\n\r\n### Behavior\r\n`now()` returns a datetime value representing the system time in UTC when the query is evaluated.  \r\nEach query execution computes `now()` once, and the value remains constant throughout that query's lifetime.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nnow()\r\n```  \r\nReturns the current UTC datetime without any arguments.  \r\n\r\n### Difference from `ago()`\r\n- `now()` returns the current timestamp, while `ago()` returns `now() - timespan`.  \r\n- `now()` is often combined with arithmetic to define custom time ranges manually.  \r\n\r\n### Performance notes\r\n`now()` is evaluated once per query execution, making it efficient and stable within that query context.  \r\nIt does not add overhead and can be reused multiple times in the same query without recomputation.  \r\n\r\n### Remarks\r\n- The value returned by `now()` is in UTC. If local time calculations are needed, adjust using arithmetic (e.g., adding an offset).  \r\n- Useful for creating dynamic time filters or generating current snapshot timestamps.  \r\n- Can be assigned to a `let` variable for clarity and reuse in larger queries.  \r\n\r\n### Typical use cases\r\n- Defining time windows dynamically (e.g., last hour, last 7 days)\r\n- Calculating durations or time differences relative to the current moment\r\n- Creating derived columns with age or latency since `now()`  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Prints the current timestamp\r\nprint now = now();",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Prints a timestamp representing 1 hour ago\r\nprint one_hour_ago1 = ago(1h), one_hour_ago2 = now(-1h)",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Prints a timestamp representing 30 minutes ago\r\nprint thirty_minutes_ago1 = ago(30m), thirty_minutes_ago2= now(-30m)",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Prints a timestamp representing 7 days ago\r\nprint seven_days_ago1 = ago(7d), seven_days_ago2 = now(-7d);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Prints a timestamp representing 1 month ago (approx. 30 days)\r\nprint one_month_ago1 = ago(30d), one_month_ago2 = now(-30d);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Calculate exactly one calendar month ago from current time\r\nprint one_month_ago = datetime_add(\"month\", -1, now());",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Calculate exactly one calendar year ago from current time\r\nprint one_year_ago = datetime_add(\"year\", -1, now())",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Add a column with current timestamp (now() is evaluated once per query execution)\r\n// Return original event time and current time columns\r\ndatabase('SecurityLogs').AuthenticationEvents\r\n| extend CurrentTime = now()\r\n| project CurrentTime",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## Date and time functions in KQL\r\n\r\nKusto provides a variety of functions for working with dates and times, including creating, manipulating, and extracting information from datetime values.  \r\n\r\n### Purpose\r\nDate functions in KQL are used to **handle, transform, and analyze datetime values**, supporting use cases like dynamic time filtering, bucketing data into time periods, or formatting for reporting.  \r\n\r\n### Core functions overview\r\n\r\n#### Getting the current date and time\r\n- `now()`: Returns the current UTC datetime when the query starts running. The value remains constant during the query.  \r\n- `ago(timespan)`: Returns the datetime representing \"now minus timespan\", useful for relative time filters (e.g., `ago(7d)` for last 7 days).  \r\n\r\n#### Creating datetime values\r\n- `datetime()`: Converts a string (usually in ISO 8601 format, e.g., `\"2025-07-08T10:30:00Z\"`) into a datetime value.  \r\n- `make_datetime()`: Constructs a datetime value from year, month, day, hour, minute, and second components.  \r\n\r\n#### Manipulating datetime values\r\n- `datetime_add(unit, value, baseDate)`: Adds a specified number of units (e.g., months, days) to a base datetime value.  \r\n- `datetime_diff(unit, date1, date2)`: Returns the difference between two datetime values in the specified unit (e.g., days, hours).  \r\n\r\n#### Extracting components from datetime values\r\n- `datetime_part(part, datetime)`: Extracts a specific component (e.g., year, month, day, hour) from a datetime value.  \r\n- `startofyear()`, `startofmonth()`, `startofday()`, etc.: Return the start of the respective time period.  \r\n- `endofyear()`, `endofmonth()`, `endofday()`, etc.: Return the end of the respective time period.  \r\n\r\n#### Formatting datetime values\r\n- `format_datetime(datetime, format)`: Converts a datetime value into a formatted string using custom format patterns.  \r\n\r\n#### Working with time zones\r\nKusto primarily uses UTC for datetime operations.  \r\nWhen working with local time or time zone offsets, you can use `todatetime()` with a timezone specifier (e.g., `\"+02:00\"`) to convert a string with an offset to UTC.  \r\n\r\n### Performance notes\r\nDatetime functions are evaluated row by row when used in `extend` or `project`, or once per query when used in `let` assignments (e.g., `let now_value = now();`).  \r\nUsing functions like `ago()` and `now()` in `where` clauses helps the engine efficiently prune data early.  \r\n\r\n### Remarks\r\n- Always consider the default UTC behavior when interpreting results or displaying to users in different time zones.  \r\n- Functions can be nested and combined for advanced date logic (e.g., `startofmonth(ago(30d))`).  \r\n- `let` assignments improve readability and avoid recalculating complex expressions.  \r\n\r\n### Typical use cases\r\n- Filtering rows from the last N days (e.g., `where Timestamp > ago(7d)`).  \r\n- Grouping data by month or week (e.g., `extend EventMonth = startofmonth(Timestamp)`).  \r\n- Calculating event durations or time differences (e.g., `datetime_diff('hour', EndTime, StartTime)`).  \r\n- Formatting timestamps for display in dashboards or export reports.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "print\r\n    StartOfToday = startofday(now()),\r\n    EndOfToday = endofday(now()), \r\n    StartOfYesterday = startofday(ago(1d)),\r\n    EndOfYesterday = endofday(ago(1d));\r\nprint    \r\n    StartOfThisWeek = startofweek(now()),\r\n    EndOfThisWeek = endofweek(now());\r\nprint    \r\n    StartOfThisMonth = startofmonth(now()),\r\n    EndOfThisMonth = endofmonth(now()),\r\n    StartOfLastMonth_1 = startofmonth(startofmonth(now()) - 1d),\r\n    StartOfLastMonth_2 = datetime_add('month', -1, startofmonth(now())),\r\n    EndOfNextMonth_1 = endofmonth(endofmonth(now()) + 1d),\r\n    EndOfNextMonth_2 = datetime_add('month', 1, endofmonth(now()));\r\nprint    \r\n    StartOfThisYear = startofyear(now()),\r\n    EndOfThisYear = endofyear(now()),\r\n    StartOfLastYear = datetime_add('year', -1, startofyear(now())),\r\n    EndOfNextYear = datetime_add('year', 1, endofyear(now()));",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// takes the first 50 rows from the table.\r\n//\r\n// Uses 'extend' to add several calculated columns related to event durations and time differences:\r\n//\r\n// - 'Duration': computes the duration of each event as 'EndTime - StartTime'.\r\n//\r\n// - 'TimeDifference': calculates the difference between 'StartTime' and the current time ('now()') as a timespan.\r\n//\r\n// - 'TimeDifferenceInDaysIndustrialTime': converts the time difference into days using \"industrial time\" notation (i.e., as a real number of days), by dividing the timespan by 1d.\r\n//\r\n// - 'TimeDifferenceInHoursIndustrialTime': converts the time difference into hours as a floating-point value, by dividing the timespan by 1h.\r\n//\r\n// - 'TimeDifferenceInHours': uses 'datetime_diff' to compute the difference between 'StartTime' and 'now()' in whole hours.\r\n//\r\n// - 'TimeDifferenceInYears': uses 'datetime_diff' to compute the difference between 'StartTime' and 'now()' in whole years.\r\n//\r\n// Finally, uses 'project' to select and display only these relevant columns in the output:\r\n//   - 'StartTime'\r\n//   - 'EndTime'\r\n//   - 'Duration'\r\n//   - 'TimeDifference'\r\n//   - 'TimeDifferenceInDaysIndustrialTime'\r\n//   - 'TimeDifferenceInHoursIndustrialTime'\r\n//   - 'TimeDifferenceInHours'\r\n//   - 'TimeDifferenceInYears'\r\ndatabase('Samples').StormEvents\r\n| take 50\r\n| extend Duration = (EndTime - StartTime)\r\n| extend TimeDifference = ((StartTime) - now())\r\n| extend TimeDifferenceInDaysIndustrialTime = ((StartTime) - now()) / 1d\r\n| extend TimeDifferenceInHoursIndustrialTime = ((StartTime) - now()) / 1h\r\n| extend TimeDifferenceInHours = datetime_diff('hour', StartTime, now())\r\n| extend TimeDifferenceInYears = datetime_diff('year', StartTime, now())\r\n| project\r\n    StartTime,\r\n    EndTime,\r\n    Duration,\r\n    TimeDifference,\r\n    TimeDifferenceInDaysIndustrialTime,\r\n    TimeDifferenceInHoursIndustrialTime,\r\n    TimeDifferenceInHours,\r\n    TimeDifferenceInYears;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the first 50 rows from the 'StormEvents' table in the 'Samples' database,\r\n// and demonstrates how to format the 'StartTime' column into various date and time string representations.\r\n//\r\n// The 'format_datetime()' function converts a datetime value into a string formatted according to a specified pattern.\r\n//\r\n// The query projects (selects) the following columns:\r\n// - 'StartTime': the original datetime value.\r\n// - Various formatted versions of 'StartTime' using different patterns:\r\n//     - \"yyyyMMdd\": e.g., 20230718\r\n//     - \"yyyy-MM-dd\": e.g., 2023-07-18\r\n//     - \"dd.MM.yyyy\": e.g., 18.07.2023\r\n//     - \"MM.yyyy\": e.g., 07.2023\r\n//     - \"dd\": day of the month only\r\n//     - \"dd.MM.yyyy hh:mm:ss\": full date with time (24-hour format without AM/PM)\r\n//     - \"dd.MM.yyyy h:mm:ss tt\": full date with time and AM/PM indicator\r\n//     - \"hh:mm:ss.ff\": hours, minutes, seconds, and fractional seconds\r\n//     - \"hh:mm\": hours and minutes (24-hour format)\r\n//     - \"mm\": minutes only\r\n//     - \"tt\": AM or PM designator\r\n//\r\n// The result shows how the same datetime can be flexibly presented in different formats for reporting or display purposes.\r\ndatabase('Samples').StormEvents\r\n| take 50\r\n| project\r\n    StartTime,\r\n    [\"yyyyMMdd\"] = format_datetime(StartTime, \"yyyyMMdd\"),\r\n    [\"yyyy-MM-dd\"] = format_datetime(StartTime, \"yyyy-MM-dd\"),\r\n    [\"dd.MM.yyyy\"] = format_datetime(StartTime, \"dd.MM.yyyy\"),\r\n    [\"MM.yyyy\"] = format_datetime(StartTime, \"MM.yyyy\"),\r\n    [\"dd\"] = format_datetime(StartTime, \"dd\"),\r\n    [\"dd.MM.yyyy hh:mm:ss\"] = format_datetime(StartTime, \"dd.MM.yyyy hh:mm:ss\"),\r\n    [\"dd.MM.yyyy h:mm:ss tt\"] = format_datetime(StartTime, \"dd.MM.yyyy h:mm:ss tt\"),    \r\n    [\"hh:mm:ss.ff\"] = format_datetime(StartTime, \"hh:mm:ss.ff\"),\r\n    [\"hh:mm\"] = format_datetime(StartTime, \"hh:mm\"),\r\n    [\"mm\"] = format_datetime(StartTime, \"mm\"),\r\n    [\"tt\"] = format_datetime(StartTime, \"tt\");",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query extracts different date components from the StartTime column for the first 50 storm events\r\n// datetime_part returns an integer\r\n// This query retrieves the first 50 rows from the 'StormEvents' table in the 'Samples' database,\r\n// and demonstrates how to extract different date components from the 'StartTime' column.\r\n//\r\n// The 'datetime_part()' function extracts a specific part of a datetime value and returns it as an integer.\r\n//\r\n// The query projects (selects) the following columns:\r\n// - 'StartTime': the original datetime value.\r\n// - 'year': the four-digit year extracted from 'StartTime'.\r\n// - 'quarter': the quarter of the year (1 to 4).\r\n// - 'month': the month of the year (1 to 12).\r\n// - 'day': the day of the month (1 to 31).\r\n// - 'week_of_year': the ISO week number of the year (1 to 53).\r\n// - 'dayofyear': the day number within the year (1 to 366).\r\n//\r\n// These extracted components are useful for grouping, filtering, or creating time-based calculations and reports.\r\ndatabase('Samples').StormEvents\r\n| take 50\r\n| project\r\n    StartTime,\r\n    [\"year\"] = datetime_part('year', StartTime),\r\n    [\"quarter\"] = datetime_part('quarter', StartTime),\r\n    [\"month\"] = datetime_part('month', StartTime),\r\n    [\"day\"] = datetime_part('day', StartTime),                        \r\n    [\"week_of_year\"] = datetime_part('week_of_year', StartTime),\r\n    [\"dayofyear\"] = datetime_part('dayofyear', StartTime);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query filters the 'StormEvents' table from the 'Samples' database to include events that occurred within September 2007.\r\n// It first defines a reference date 'TheDate' as September 5, 2007, using 'make_datetime'.\r\n// Then, it calculates:\r\n//   - 'StartOfYear': first day of the year containing 'TheDate'.\r\n//   - 'EndOfYear': last day of the year containing 'TheDate'.\r\n//   - 'StartOfMonth': first day of September 2007.\r\n//   - 'EndOfMonth': last day of September 2007.\r\n//   - 'StartOfWeek': first day of the week containing 'TheDate'.\r\n//   - 'EndOfWeek': last day of that week.\r\n//\r\n// It filters events to those where 'StartTime' falls within September 2007 (between 'StartOfMonth' and 'EndOfMonth').\r\n// Finally, it projects these computed date columns and the original 'StartTime', and limits the output to 100 rows.\r\ndatabase('Samples').StormEvents\r\n| extend TheDate = make_datetime(2007, 9, 5)\r\n| extend StartOfYear = startofyear(TheDate)\r\n| extend EndOfYear = endofyear(TheDate)\r\n| extend StartOfMonth = startofmonth(TheDate)\r\n| extend EndOfMonth = endofmonth(TheDate)\r\n| extend StartOfWeek = startofweek(TheDate)\r\n| extend EndOfWeek = endofweek(TheDate)\r\n| where StartTime between (StartOfMonth .. EndOfMonth)\r\n| project\r\n    TheDate,\r\n    StartOfYear,\r\n    EndOfYear,\r\n    StartOfMonth,\r\n    EndOfMonth,\r\n    StartOfWeek,\r\n    EndOfWeek,\r\n    StartTime\r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves storm event records from the 'StormEvents' table in the 'Samples' database.\r\n//\r\n// It uses 'extend' to add a new column 'StartMonth', which represents the first day of the month when each event started,\r\n// calculated using the 'startofmonth(StartTime)' function.\r\n//\r\n// Then, it summarizes (aggregates) the data by counting the number of events ('NumberOfEvents')\r\n// for each combination of 'State' and 'StartMonth'.\r\n//\r\n// Finally, it sorts the summarized results in ascending order first by 'StartMonth', and then by 'State'.\r\n//\r\n// The result shows how many storm events occurred in each state per month.\r\ndatabase('Samples').StormEvents\r\n| extend StartMonth = startofmonth(StartTime)\r\n| summarize NumberOfEvents = count() by State, StartMonth\r\n| sort by StartMonth, State asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the start times of storm events from the 'StormEvents' table in the 'Samples' database.\r\n//\r\n// It filters the events to include only those where 'StartTime' falls between October 1, 2007 (2007101), and December 31, 2007.\r\n// The 'todatetime()' function converts the numeric strings into datetime values for filtering.\r\n//\r\n// Then, it uses 'extend' to extract the day component (1 to 31) from each event's 'StartTime' using 'datetime_part(\"day\", StartTime)'.\r\n//\r\n// After that, it summarizes the data by counting the number of events ('NumberOfEvents') for each day value,\r\n// regardless of the month.\r\n//\r\n// Finally, it orders the results in ascending order by 'Day'.\r\n//\r\n// The result shows how many storm events occurred on each day of the month, aggregated across the filtered date range.\r\ndatabase('Samples').StormEvents\r\n| project StartTime\r\n| where StartTime between (todatetime(\"2007101\") .. todatetime(\"20071231\"))\r\n| extend Day = datetime_part('day', StartTime)\r\n| summarize NumberOfEvents = count() by Day\r\n| order by Day asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the start times of storm events from the 'StormEvents' table in the 'Samples' database.\r\n//\r\n// It filters the events to include only those where 'StartTime' falls within the year 2007,\r\n// using explicit datetime literals (e.g., datetime(2007-01-01)) instead of 'todatetime()'.\r\n//\r\n// Then, it uses 'extend' to extract the day component (1 to 31) from each event's 'StartTime' using 'datetime_part(\"day\", StartTime)'.\r\n//\r\n// Next, it summarizes the data by counting the number of events ('NumberOfEvents') for each day value,\r\n// regardless of the month (i.e., it aggregates by day-of-month across the entire year).\r\n//\r\n// Finally, it orders the results in ascending order by 'Day'.\r\n//\r\n// The result shows how many storm events occurred on each day of the month (1–31), summed across all months of 2007.\r\ndatabase('Samples').StormEvents\r\n| project StartTime\r\n| where StartTime between (datetime(2007-01-01) .. datetime(2007-12-31))\r\n| extend Day = datetime_part('day', StartTime)\r\n| summarize NumberOfEvents = count() by Day\r\n| order by Day asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query filters the 'StormEvents' table from the 'Samples' database to include events occurring in September 2007.\r\n//\r\n// It first defines a reference date 'TheDate' as September 5, 2007, using 'make_datetime'.\r\n//\r\n// Then, it calculates several date boundaries based on this reference date:\r\n// - 'StartOfYear': the first day of 2007.\r\n// - 'EndOfYear': the last day of 2007.\r\n// - 'StartOfMonth': the first day of September 2007.\r\n// - 'EndOfMonth': the last day of September 2007.\r\n// - 'StartOfWeek': the first day of the week containing September 5, 2007.\r\n// - 'EndOfWeek': the last day of that same week.\r\n//\r\n// It filters the events to include only those where 'StartTime' falls within the range from 'StartOfMonth' to 'EndOfMonth'.\r\n//\r\n// Finally, it projects (selects) these calculated date columns along with the event's 'StartTime',\r\n// and limits the output to the first 100 rows.\r\ndatabase('Samples').StormEvents\r\n| extend TheDate = make_datetime(2007, 9, 5)\r\n| extend StartOfYear = startofyear(TheDate)\r\n| extend EndOfYear = endofyear(TheDate)\r\n| extend StartOfMonth = startofmonth(TheDate)\r\n| extend EndOfMonth = endofmonth(TheDate)\r\n| extend StartOfWeek = startofweek(TheDate)\r\n| extend EndOfWeek = endofweek(TheDate)\r\n| where StartTime between (StartOfMonth .. EndOfMonth)\r\n| project\r\n    TheDate,\r\n    StartOfYear,\r\n    EndOfYear,\r\n    StartOfMonth,\r\n    EndOfMonth,\r\n    StartOfWeek,\r\n    EndOfWeek,\r\n    StartTime\r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `project` operator in KQL\r\n\r\n### Purpose\r\nThe `project` operator is used to **select and shape columns** from a table or query result.  \r\nIt allows you to control which columns are included, rename columns, and create calculated columns in the output.  \r\n\r\n### Behavior\r\nBy default, only the columns listed in `project` are included in the result.  \r\nColumns can be renamed using `NewName = Expression`.  \r\nYou can also define calculated columns directly within `project`.  \r\nIf you want to keep all existing columns and add new ones, use `project-extend` instead.  \r\n\r\n### Difference from `extend`\r\n`extend` adds new columns but keeps all existing columns.  \r\n`project` reduces the output to only the specified columns (and can create new ones as needed).  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| project ColumnA, ColumnB\r\n```  \r\n\r\n### Performance notes\r\nUsing `project` early in a query can improve performance, as it reduces the data volume flowing to subsequent steps.  \r\nIt helps optimize queries by minimizing the amount of data carried forward.  \r\n\r\n### Remarks\r\n- `project` does not filter rows, only columns.  \r\n- If a column name is misspelled or does not exist, the query will fail at runtime.  \r\n- `project` is commonly used at the end of queries to shape the final result set.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// takes the first 5 arbitrary rows,\r\n// and then selects (projects) only the columns EventId, State, and EventType for the output.\r\ndatabase('Samples').StormEvents\r\n| take 5\r\n| project EventId, State, EventType;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table,\r\n// creates a new column 'StartLocation' by renaming 'BeginLocation',\r\n// creates a new column 'TotalInjuries' as the sum of 'InjuriesDirect' and 'InjuriesIndirect',\r\n// then filters rows where 'TotalInjuries' is greater than 5,\r\n// and finally takes up to 5 results.\r\ndatabase('Samples').StormEvents\r\n| project StartLocation = BeginLocation, TotalInjuries = InjuriesDirect + InjuriesIndirect\r\n| where TotalInjuries > 5\r\n| take 5",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table,\r\n// keeps only the columns 'BeginLocation', 'InjuriesDirect', and 'InjuriesIndirect',\r\n// then filters rows where the sum of 'InjuriesDirect' and 'InjuriesIndirect' is greater than 5,\r\n// takes up to 5 results,\r\n// and finally creates two new columns:\r\n//   'StartLocation', which is just 'BeginLocation' renamed,\r\n//   and 'TotalInjuries', which is the sum of 'InjuriesDirect' and 'InjuriesIndirect'.\r\ndatabase('Samples').StormEvents\r\n| project BeginLocation, InjuriesDirect, InjuriesIndirect\r\n| where (InjuriesDirect + InjuriesIndirect) > 5\r\n| take 5\r\n| project StartLocation = BeginLocation, TotalInjuries = InjuriesDirect + InjuriesIndirect",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "### Query 1\r\n\r\n```kql\r\nStormEvents\r\n| project StartLocation = BeginLocation, TotalInjuries = InjuriesDirect + InjuriesIndirect\r\n| where TotalInjuries > 5\r\n| take 5\r\n```\r\n\r\n### Query 2\r\n\r\n```kql\r\nStormEvents\r\n| project BeginLocation, InjuriesDirect, InjuriesIndirect\r\n| where (InjuriesDirect + InjuriesIndirect) > 5\r\n| take 5\r\n| project StartLocation = BeginLocation, TotalInjuries = InjuriesDirect + InjuriesIndirect\r\n```\r\n\r\n### Performance difference\r\n\r\n#### Query 1\r\n\r\n* Immediately reduces columns to `StartLocation` and `TotalInjuries` at the start.\r\n* Filters using `TotalInjuries`.\r\n* Less column data carried forward; simpler, fewer expressions repeated.\r\n\r\n#### Query 2\r\n\r\n* Keeps raw columns (`BeginLocation`, `InjuriesDirect`, `InjuriesIndirect`) at first.\r\n* Calculates `InjuriesDirect + InjuriesIndirect` only in the `where` clause.\r\n* Calculates `StartLocation` and `TotalInjuries` **again** at the final `project`.\r\n* More intermediate columns are carried until the last step.",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table,\r\n// creates a new column 'StartLocation' by renaming 'BeginLocation',\r\n// creates a new column 'TotalInjuries' as the sum of 'InjuriesDirect' and 'InjuriesIndirect',\r\n// then tries to filter on 'BeginLocation' — but this column no longer exists after the project step (it was renamed to 'StartLocation'),\r\n// so the query fails with:\r\n// Request is invalid and cannot be executed. ('where' operator: Failed to resolve column or scalar expression named 'BeginLocation')\r\ndatabase('Samples').StormEvents\r\n| project StartLocation = BeginLocation, TotalInjuries = InjuriesDirect + InjuriesIndirect\r\n| where BeginLocation == 'ROYAL'\r\n| take 5",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `project-away` operator in KQL\r\n\r\n### Purpose\r\nThe `project-away` operator is used to **remove (drop) specific columns** from the result set.  \r\nIt is effectively the opposite of `project`, which selects columns to keep.  \r\n\r\n### Behavior\r\nYou list the columns you want to exclude from the output.  \r\nAll other columns will be included.  \r\n\r\n### Difference from `project`\r\n- `project` explicitly selects the columns you want to keep.  \r\n- `project-away` explicitly specifies the columns you want to remove, keeping everything else.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| project-away ColumnA, ColumnB\r\n```  \r\nThis keeps all columns **except** `ColumnA` and `ColumnB`.  \r\n\r\n### Performance notes\r\nUsing `project-away` can help reduce data transferred downstream when you only want to remove a few columns from wide tables.  \r\nIt can simplify queries when most columns should stay and only a few need to be excluded.  \r\n\r\n### Remarks\r\n- `project-away` does not affect rows; it only removes columns.  \r\n- If a specified column does not exist, the query will fail at runtime.  \r\n- It is especially useful when working with tables that have many columns (e.g., logs or telemetry data).  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Shows metadata information for the table 'PopulationData' in the current database,\r\n// including column names, data types, and additional properties (such as table policies and storage details).\r\n.show table PopulationData;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'PopulationData' table in the 'Samples' database,\r\n// removes (drops) the column 'Population' from the result set,\r\n// and keeps all other columns.\r\ndatabase('Samples').PopulationData\r\n| project-away Population;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `project-keep` operator in KQL\r\n\r\n### Purpose\r\nThe `project-keep` operator is used to **keep only specific columns** from the input data set.  \r\nIt is functionally similar to `project`, but slightly different in intent and usage clarity.  \r\n\r\n### Behavior\r\nYou list the columns you want to keep, and all other columns are removed.  \r\nUnlike `project`, `project-keep` does not allow you to create new columns or expressions — it strictly keeps the columns as they are.  \r\n\r\n### Difference from `project`\r\n- `project` allows selecting, renaming, and creating new calculated columns.  \r\n- `project-keep` only keeps existing columns without any modifications.  \r\n\r\n### Syntax\r\n```kql\r\nTableName\r\n| project-keep ColumnA, ColumnB\r\n```  \r\nThis keeps only `ColumnA` and `ColumnB`, and drops all other columns.  \r\n\r\n### Performance notes\r\nUsing `project-keep` early can improve performance by reducing the data size passed to subsequent operators.  \r\nIt is especially useful when working with wide tables and you only need a few columns.  \r\n\r\n### Remarks\r\n- `project-keep` does not remove or filter rows — it only affects columns.  \r\n- If a specified column does not exist, the query will fail at runtime.  \r\n- Helps clarify intent when the goal is to keep columns as-is, rather than creating or renaming them.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Shows metadata information for the table 'PopulationData' in the current database,\r\n// including column names, data types, and additional properties (such as table policies and storage details).\r\n.show table PopulationData;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'PopulationData' table in the 'Samples' database,\r\n// keeps only the column 'Population' in the result set,\r\n// and removes all other columns.\r\ndatabase('Samples').PopulationData\r\n| project-keep Population;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `project-rename` operator in KQL\r\n\r\n### Purpose\r\nThe `project-rename` operator is used to **rename one or more columns** in the result set.  \r\nIt is useful when you want to keep all columns but change their names for clarity or presentation purposes.  \r\n\r\n### Behavior\r\nYou specify the new column names and map them to existing columns.  \r\nThe data in the columns remains unchanged; only the column headers are updated.  \r\n\r\n### Difference from `project`\r\n- `project` allows selecting, renaming, and creating new columns all at once.  \r\n- `project-rename` focuses only on renaming existing columns and keeps all other columns as they are.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| project-rename NewName = OldName\r\n```  \r\nThis keeps all columns but renames `OldName` to `NewName`.  \r\n\r\n### Performance notes\r\nRenaming columns has negligible impact on performance since no data is transformed, only metadata is updated.  \r\n\r\n### Remarks\r\n- `project-rename` does not remove or filter columns or rows — it only renames columns.  \r\n- If a specified column does not exist, the query will fail at runtime.  \r\n- Especially useful when preparing results for reports or exporting to external tools that require more descriptive column names.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'PopulationData' table in the 'Samples' database,\r\n// renames the column 'State' to 'NameOfState',\r\n// while keeping all other columns unchanged if any,\r\n// Project-rename renames columns but does not remove any columns.\r\ndatabase('Samples').PopulationData\r\n| project-rename NameOfState = State;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `project-reorder` operator in KQL\r\n\r\n### Purpose\r\nThe `project-reorder` operator is used to **change the order of columns** in the result set.  \r\nIt is useful for controlling the presentation of data, for example when preparing results for reports or export.  \r\n\r\n### Behavior\r\nYou list the columns in the desired new order.  \r\nAny columns not listed explicitly will appear after the specified columns, preserving their original relative order.  \r\n\r\n### Difference from `project`\r\n- `project` defines which columns to include and can create or rename columns.  \r\n- `project-reorder` only changes the display order of existing columns without affecting their names or data.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| project-reorder ColumnA, ColumnB\r\n```  \r\nThis moves `ColumnA` and `ColumnB` to the front, and the remaining columns follow in their original order.  \r\n\r\n### Performance notes\r\nReordering columns has negligible or no impact on performance because it is purely a presentation-level change.  \r\n\r\n### Remarks\r\n- `project-reorder` does not remove, filter, or rename columns — it only changes their order.  \r\n- If a specified column does not exist, the query will fail at runtime.  \r\n- Useful for improving readability or matching required column order in exported data or downstream tools.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// takes 5 arbitrary rows,\r\n// then reorders the columns so that 'EventType', 'BeginLocation', 'EndLocation', 'State', and 'EpisodeNarrative' appear first,\r\n// and any remaining columns follow in their original order.\r\ndatabase('Samples').StormEvents\r\n| take 5\r\n| project-reorder EventType, BeginLocation, EndLocation, State, EpisodeNarrative;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table in the 'Samples' database,\r\n// takes 5 arbitrary rows,\r\n// then reorders all columns alphabetically in ascending order using '* asc'.\r\ndatabase('Samples').StormEvents\r\n| take 5\r\n| project-reorder * asc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `extend` operator in KQL\r\n\r\n### Purpose\r\nThe `extend` operator is used to **add new calculated columns** to the result set.  \r\nIt allows you to create additional columns based on expressions or transformations of existing data, without removing any original columns.  \r\n\r\n### Behavior\r\nNew columns are added at the end of the schema.  \r\nExisting columns remain unchanged and are included in the output by default.  \r\nYou can define one or multiple new columns in a single `extend` statement.  \r\n\r\n### Difference from `project`\r\n- `extend` keeps all existing columns and simply adds new ones.  \r\n- `project` can select, rename, and create columns, but only outputs the columns you explicitly specify.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| extend NewColumn = Expression\r\n```  \r\nThis adds `NewColumn` to the result set.  \r\n\r\n### Performance notes\r\nUsing `extend` can slightly increase query cost if the expressions are computationally expensive, as new columns are evaluated for each row.  \r\nHowever, it does not affect the number of rows and generally has low overhead when expressions are simple.  \r\n\r\n### Remarks\r\n- `extend` does not filter or remove rows or columns.  \r\n- If a new column name already exists, it will overwrite (shadow) the existing column in the query output.  \r\n- Useful for adding derived metrics, intermediate calculations, or debug columns during analysis.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'StormEvents' table,\r\n// takes 5 arbitrary rows,\r\n// then selects the columns 'EventType', 'State', 'StartTime', and 'EndTime',\r\n// and finally adds a new column 'Duration' that calculates the difference between 'EndTime' and 'StartTime' in minutes.\r\ndatabase('Samples').StormEvents\r\n| take 5\r\n| project EventType, State, StartTime, EndTime\r\n| extend Duration = datetime_diff('Minute', EndTime, StartTime);",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `sort` operator in KQL\r\n\r\n### Purpose\r\nThe `sort` operator is used to **order rows in the result set based on one or more columns**, either in ascending or descending order.  \r\nIt is essential for arranging data when presenting results, applying row-based functions, or preparing datasets for further analysis.\r\n\r\n### Behavior\r\nThe operator sorts the rows according to the specified columns. By default, sorting is in ascending order unless `desc` is explicitly specified.  \r\nWhen sorting by multiple columns, it orders first by the first column, then breaks ties using the next columns in order.\r\n\r\n### Syntax\r\n\r\n```kql\r\nT | sort [asc|desc] by Column1, Column2, ...\r\n```  \r\n- `Column1, Column2, ...`: Columns to sort by, listed in priority order.  \r\n- You can specify `asc` (ascending) or `desc` (descending) for each column.\r\n\r\n### Performance notes\r\nSorting is a resource-intensive operation, especially on large datasets.  \r\nMinimize the number of columns and rows when sorting, and use pre-filtering (`where`) when possible to improve performance.\r\n\r\n### Remarks\r\n- Equivalent to SQL `ORDER BY`, but always explicit about sorting direction.  \r\n- Often used before applying row-based functions like `prev()`, `next()`, or `row_cumsum()`, since these depend on a defined row order.  \r\n- Sorting does not reduce the number of rows; it only reorders them.  \r\n- If deterministic ordering is not needed, `serialize` can be used to preserve existing order without physically re-sorting.  \r\n- Combining `sort` with `take` is common to retrieve top or bottom N rows efficiently.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes data from the 'StormEvents' table in the 'Samples' database,\r\n// focusing on identifying all unique event types that occurred in the state of \"IOWA\".\r\n//\r\n// 1️⃣ The `where` operator filters the dataset to include only rows where 'State' is equal to \"IOWA\".\r\n//\r\n// 2️⃣ The `distinct` operator returns a unique list of values from the 'EventType' column\r\n//     for the filtered rows — eliminating any duplicates.\r\n//\r\n// 3️⃣ The `sort by` operator then orders these distinct event types alphabetically in ascending order.\r\n//\r\n// ✅ This approach is useful for generating a clean, ordered list of event types for a specific region,\r\n// which can help when building dropdown filters or summary reports.\r\ndatabase('Samples').StormEvents\r\n| where State == \"IOWA\"\r\n| distinct EventType\r\n| sort by EventType asc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `top` operator in KQL\r\n\r\n### Purpose\r\nThe `top` operator is used to **return a specified number of rows with the highest values in one or more columns**.  \r\nIt is commonly used to identify top performers, highest counts, largest amounts, or other top-ranking items in your data.\r\n\r\n### Behavior\r\nThe `top` operator sorts the data by one or more columns (by default in descending order) and returns only the first N rows.  \r\nIf two or more rows have the same value at the boundary, they are included in the result, which may result in more than N rows.\r\n\r\n### Syntax\r\n\r\n```kql\r\nT | top N by Column1 [desc|asc], Column2 [desc|asc], ...\r\n```  \r\n- `N`: The number of top rows to return.  \r\n- `Column1, Column2, ...`: Columns to sort by and determine \"topness\".  \r\n- You can specify `asc` or `desc` to control sort direction for each column (default is `desc`).\r\n\r\n### Performance notes\r\nThe `top` operator is optimized to avoid fully sorting the entire dataset when possible, improving efficiency on large datasets.  \r\nHowever, performance can still be impacted if many columns are involved or if there are large numbers of ties at the cutoff.\r\n\r\n### Remarks\r\n- Equivalent to `sort by ... | take N`, but with optimizations for performance and potential tie handling.  \r\n- Can be used with `summarize` to first aggregate data and then pick top groups.  \r\n- If you want strict control over tie-breaking, consider adding additional columns to the `by` clause to define order precisely.  \r\n- When multiple columns are specified, rows are sorted by the first column, then by the second in case of ties, and so on.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes data from the 'SalesFact' table in the 'ContosoSales' database,\r\n// focusing on identifying the highest sales transactions.\r\n//\r\n// 1️⃣ The `top` operator is used to return only the top 10 rows based on the 'SalesAmount' column.\r\n//\r\n// 2️⃣ The `by SalesAmount desc` clause specifies that rows should be sorted in descending order by 'SalesAmount':\r\n//     - This means the rows with the highest sales amounts will appear at the top of the result.\r\n//\r\n// ✅ The `top` operator is functionally similar to using `sort by SalesAmount desc | take 10`,\r\n// but it is optimized for performance and can handle ties at the boundary if necessary.\r\n//\r\n// This approach is useful for quickly identifying your best-selling transactions or biggest deals.\r\ndatabase('ContosoSales').SalesFact\r\n| top 10 by SalesAmount desc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes customer sales performance in the 'SalesFact' table of the 'ContosoSales' database.\r\n//\r\n// 1️⃣ The `project` operator selects only the 'CustomerKey' and 'SalesAmount' columns for further analysis.\r\n//\r\n// 2️⃣ The `summarize` operator groups the data by 'CustomerKey' and calculates the total sales amount per customer\r\n//     using `sum(SalesAmount)`. The result is stored in a new column named 'TotalSaels'.\r\n//     ⚠️ Note: There is a typo here — \"TotalSaels\" should likely be \"TotalSales\".\r\n//\r\n// 3️⃣ The `top` operator then returns the top 10 customers based on 'TotalSaels', sorted in descending order.\r\n//\r\n// ✅ This approach is useful for quickly identifying your top 10 customers by total purchase volume.\r\n// It helps in prioritizing high-value customers for marketing or support activities.\r\ndatabase('ContosoSales').SalesFact\r\n| project CustomerKey, SalesAmount\r\n| summarize TotalSaels = sum(SalesAmount) by CustomerKey\r\n| top 10 by TotalSaels desc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `summarize` operator in KQL\r\n\r\n### Purpose\r\nThe `summarize` operator is used to **group rows and calculate aggregations** over each group.  \r\nIt is similar to the `GROUP BY` clause in SQL and is fundamental for producing aggregated insights from data.  \r\n\r\n### Behavior\r\nYou define one or more aggregation functions (like `count()`, `avg()`, `sum()`, etc.) and specify grouping columns.  \r\nThe output contains one row per unique combination of the grouping columns.  \r\n\r\n### Grouping columns\r\nIf you specify columns after `by`, the data is grouped by these columns.  \r\nIf no grouping columns are specified, all data is aggregated into a single result row.  \r\n\r\n### Difference from other operators\r\n- Unlike `project` or `extend`, which work row by row, `summarize` reduces multiple rows into a single summary row per group.  \r\n- Can be combined with `top`, `order by`, or further filtering to analyze results.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| summarize AggregationFunction() by GroupColumn\r\n```  \r\n\r\n### Performance notes\r\n`summarize` can significantly reduce data volume by grouping and aggregating early.  \r\nIf used with large datasets, indexing or partitioning can improve its efficiency.  \r\nThe operator works in distributed fashion and can benefit from parallelism.  \r\n\r\n### Remarks\r\n- Columns not included in the grouping clause cannot be referenced outside of aggregation functions.  \r\n- The result schema only includes grouping columns and aggregation columns.  \r\n- You can use multiple aggregation functions in the same `summarize` statement.  \r\n- Useful for reporting, dashboards, and analytics queries where you need to calculate totals, averages, or other metrics.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves sales data from the 'SalesFact' table in the 'ContosoSales' database.\r\n//\r\n// 1️⃣ It uses the `where` clause to filter rows where 'DateKey' falls between April 1, 2007, and April 30, 2007\r\n//     (both dates inclusive).\r\n//\r\n// 2️⃣ The `summarize by` operator groups the remaining records by 'DateKey'.\r\n//     - This results in one row per unique date found in the filtered data.\r\n//     - No aggregation functions are specified here, so it effectively acts as a \"distinct\" grouping on the date.\r\n//\r\n// This approach is helpful when you want to generate a list of unique dates with data within a specific range,\r\n// often as a first step before adding additional aggregations or metrics.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-04-30\"))\r\n| summarize by DateKey;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves sales data from the 'SalesFact' table in the 'ContosoSales' database,\r\n// filtered to the period between April 1, 2007, and May 31, 2007.\r\n// It groups records by DateKey, CustomerKey, and ProductKey,\r\n// then calculates the total sales amount (sum of SalesAmount) for each group.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| summarize TotalSales = sum(SalesAmount) by DateKey, CustomerKey, ProductKey;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query groups sales data by DateKey, CustomerKey, and ProductKey for the specified date range,\r\n// then finds the maximum SalesAmount within each group.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\")) \r\n| summarize MaxSales = max(SalesAmount) by DateKey, CustomerKey, ProductKey;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `arg_max()` aggregation function in KQL\r\n\r\n### Purpose\r\nThe `arg_max()` aggregation function is used to **return the row that has the maximum value of a specified expression**, together with selected columns from that row.  \r\nIt is particularly useful for identifying the \"winning\" row within a grouped context (e.g., the most recent event per user or the largest transaction per customer).  \r\n\r\n### Behavior\r\nWhen used inside a `summarize` clause, `arg_max()` identifies the row with the maximum value of a specified column and retrieves one or more columns from that row.  \r\nUnlike a simple `max()` aggregation, `arg_max()` retains the context of other columns from the same row.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nsummarize arg_max(MaxColumn, OutputColumn1 [, OutputColumn2, ...]) by GroupColumns\r\n```  \r\n- `MaxColumn`: The column to determine the maximum value.\r\n- `OutputColumn1, OutputColumn2, ...`: The columns to return from the row where `MaxColumn` is maximum.\r\n- `GroupColumns`: Columns to group the data by.\r\n\r\n### Performance notes\r\n- Efficient when used with indexed columns or pre-filtered datasets.\r\n- Avoid using too many output columns unnecessarily, as this may increase memory and computation costs.\r\n\r\n### Remarks\r\n- The row chosen by `arg_max()` is non-deterministic when there are ties (multiple rows with the same max value).\r\n- Often used to simplify top-N queries or to retrieve full row context for the \"best\" or \"most recent\" entry.\r\n- You can use `arg_min()` similarly to retrieve rows with the minimum value.",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query finds the row with the maximum SalesAmount for the date range April 1 – May 31, 2007,\r\n// and returns all columns of that row (indicated by '*').\r\n// The 'arg_max' function returns the entire row where SalesAmount is maximum.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| summarize MaxSales = arg_max(SalesAmount, *);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Similar to the previous example, but here the additional columns DateKey and CustomerKey are explicitly specified\r\n// as columns to return in the row where SalesAmount is maximum.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| summarize MaxSales = arg_max(SalesAmount, DateKey, CustomerKey);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query filters first by CustomerKey = 107 and the specified date range.\r\n// Then it retrieves the row with the maximum SalesAmount, returning DateKey and CustomerKey.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\")) \r\n| where CustomerKey == 107\r\n| summarize MaxSales = arg_max(SalesAmount, DateKey, CustomerKey);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query first finds the row with maximum SalesAmount (including DateKey and CustomerKey),\r\n// then filters those results to only include rows with CustomerKey = 107.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\")) \r\n| summarize MaxSales = arg_max(SalesAmount, DateKey, CustomerKey)\r\n| where CustomerKey == 107;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `sumif()` aggregation function in KQL\r\n\r\n### Purpose\r\nThe `sumif()` aggregation function is used to **calculate the sum of an expression, but only for rows that satisfy a given condition (predicate)**.  \r\nThis function is useful for conditional aggregations, allowing you to sum only selected subsets of data directly within the summarize clause.\r\n\r\n### Behavior\r\nWhen used inside a `summarize` clause, `sumif()` evaluates the predicate for each row.  \r\nOnly rows for which the predicate evaluates to true contribute to the sum of the specified expression.\r\n\r\n### Syntax\r\n\r\n```kql\r\nsummarize sumif(Expression, Predicate) by GroupColumns\r\n```  \r\n- `Expression`: The numeric expression to sum.\r\n- `Predicate`: A boolean expression that determines whether the row is included in the sum.\r\n- `GroupColumns`: One or more columns to group the data by.\r\n\r\n### Performance notes\r\n- Using `sumif()` can improve performance and readability by avoiding the need for separate `where` filtering before aggregation.\r\n- The predicate is evaluated per row and does not add significant overhead compared to separate filtering.\r\n\r\n### Remarks\r\n- Unlike `sum()`, `sumif()` enables more precise control when combining multiple conditional aggregates in the same summarize block.\r\n- Commonly used in scenarios like \"sum sales amounts for a specific product category\" or \"sum events with severity above a threshold\".\r\n- Works well with other aggregation functions to produce concise, multi-metric outputs.",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query groups sales data from April and May 2007 by MonthOfYear,\r\n// then sums only those SalesAmount values where the month is April (4) using sumif.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| summarize TotalSales = sumif(SalesAmount, getmonth(DateKey) == 4) by MonthOfYear = getmonth(DateKey);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves all records for April 1, 2007,\r\n// then groups them by ProductKey and collects all associated CustomerKeys into a list.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey == datetime(\"2007-04-01\")\r\n| summarize \r\n    ListOfCustomerKeys = make_list(CustomerKey)\r\n    by ProductKey;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `make_list()` aggregation function in KQL\r\n\r\n### Purpose\r\nThe `make_list()` aggregation function is used to **create a dynamic array (list) containing all values of a specified expression from grouped rows**.  \r\nThis is useful for collecting detailed data points into a single array for each group in the result set.\r\n\r\n### Behavior\r\nWhen used inside a `summarize` clause, `make_list()` collects all values of an expression for each group into a dynamic array.  \r\nYou can optionally specify a maximum number of elements to include in the array (default is 1048576 elements).\r\n\r\n### Syntax\r\n\r\n```kql\r\nsummarize make_list(Expression [, MaxSize]) by GroupColumns\r\n```  \r\n- `Expression`: The column or expression whose values will be included in the list.\r\n- `MaxSize` (optional): Limits the maximum number of elements in the list (useful for controlling resource usage).\r\n- `GroupColumns`: One or more columns to group by.\r\n\r\n### Performance notes\r\n- The function is memory-intensive when creating large lists, so consider using `MaxSize` to limit the number of collected items.\r\n- Suitable for post-processing or exporting grouped detailed data.\r\n\r\n### Remarks\r\n- You can use `make_list()` when you need to analyze or export all collected values per group (e.g., all customer IDs who bought a product).\r\n- If you only need unique values, use `make_set()` instead.\r\n- Works well with other summarize aggregations to include raw value collections alongside computed metrics.",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves all records for April 1, 2007,\r\n// then groups them by ProductKey and collects all associated CustomerKeys into a list.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey == datetime(\"2007-04-01\")\r\n| summarize \r\n    ListOfCustomerKeys = make_list(CustomerKey)\r\n    by ProductKey;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query collects CustomerKeys into a unique set per ProductKey (no duplicates),\r\n// for records on April 1, 2007.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey == datetime(\"2007-04-01\")\r\n| summarize \r\n    SetOfCustomerKeys = make_set(CustomerKey)\r\n    by ProductKey;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Same as the previous example, but limits the set to a maximum of 5 unique CustomerKeys per ProductKey.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey == datetime(\"2007-04-01\")\r\n| summarize \r\n    SetOfCustomerKeys = make_set(CustomerKey, 5)\r\n    by ProductKey;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves sales data from the 'SalesFact' table in the 'ContosoSales' database.\r\n//\r\n// 1️⃣ It uses a `where` clause to filter rows where 'DateKey' is between April 1, 2007, and May 31, 2007.\r\n//\r\n// 2️⃣ The `summarize` operator aggregates the data for each group defined by:\r\n//     - 'CustomerKey'.\r\n//     - 'MonthAndYear', which is computed as (Year * 100 + Month), resulting in a format like YYYYMM.\r\n//       This is derived using `getyear(DateKey)` and `monthofyear(DateKey)`.\r\n//\r\n// 3️⃣ For each group, the following metrics are calculated:\r\n//     - `TotalSalesAmount`: The sum of 'SalesAmount'.\r\n//     - `AverageSalesAmount`: The average of 'SalesAmount'.\r\n//     - `MinimumSalesAmount`: The minimum sales amount within the group.\r\n//     - `MaximumSalesAmount`: The maximum sales amount within the group.\r\n//\r\n// 4️⃣ The results are then sorted by 'TotalSalesAmount' in ascending order.\r\n//\r\n// This query is useful for generating monthly customer-level sales summaries and identifying top or bottom performing customers based on sales volume.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| summarize\r\n    TotalSalesAmount = sum(SalesAmount),\r\n    AverageSalesAmount = avg(SalesAmount),\r\n    MinimumSalesAmount = min(SalesAmount),\r\n    MaximumSalesAmount = max(SalesAmount)\r\n    by\r\n        CustomerKey,\r\n        MonthAndYear = getyear(DateKey) * 100 + monthofyear(DateKey)\r\n| order by TotalSalesAmount;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves sales data from the 'SalesFact' table in the 'ContosoSales' database,\r\n// filtering for records where 'DateKey' is between April 1, 2007, and May 31, 2007.\r\n//\r\n// 1️⃣ The `summarize` operator aggregates sales metrics for each group defined by:\r\n//     - 'CustomerKey'.\r\n//     - 'DateKey' grouped into 5-day intervals using the `bin()` function.\r\n//       This groups dates into buckets like Apr 1–5, Apr 6–10, etc.\r\n//\r\n// 2️⃣ For each group, it calculates the following metrics:\r\n//     - `TotalSalesAmount`: Total sales amount (sum).\r\n//     - `AverageSalesAmount`: Average sales amount.\r\n//     - `MinimumSalesAmount`: Smallest individual sales amount.\r\n//     - `MaximumSalesAmount`: Largest individual sales amount.\r\n//\r\n// 3️⃣ The results are then sorted in ascending order based on 'TotalSalesAmount'.\r\n//\r\n// This query is useful for analyzing customer purchasing behavior over time,\r\n// providing insight into spending patterns in smaller, consistent time buckets.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| summarize\r\n    TotalSalesAmount = sum(SalesAmount),\r\n    AverageSalesAmount = avg(SalesAmount),\r\n    MinimumSalesAmount = min(SalesAmount),\r\n    MaximumSalesAmount = max(SalesAmount)\r\n    by\r\n        CustomerKey,\r\n        bin(DateKey, 5d)\r\n| sort by TotalSalesAmount asc",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves a single arbitrary row from the 'StormEvents' table in the 'Samples' database.\r\n//\r\n// 1️⃣ The `summarize any(*)` operator returns one arbitrary (non-deterministic) value for each column,\r\n// effectively producing just one row that represents a sample snapshot of the table.\r\n//\r\n// ✅ Useful when you just want to quickly inspect a random example row without caring which one.\r\ndatabase('Samples').StormEvents\r\n| summarize any(*);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves an arbitrary EventType from the 'StormEvents' table.\r\n//\r\n// 1️⃣ The `summarize any(EventType)` operator returns one random (non-deterministic) EventType value from all rows.\r\n//\r\n// ✅ Useful when you want to quickly check a single sample value from a column without running a distinct or full aggregation.\r\ndatabase('Samples').StormEvents\r\n| summarize any(EventType);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves an arbitrary value combination of EventType and State from the 'StormEvents' table.\r\n//\r\n// 1️⃣ The `summarize any(EventType, State)` operator returns one arbitrary (non-deterministic) combination of these two columns.\r\n//\r\n// ✅ Handy to quickly look at a sample pair of values together (e.g., to understand potential value patterns).\r\ndatabase('Samples').StormEvents\r\n| summarize any(EventType, State);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query returns one arbitrary row for each distinct EventType.\r\n//\r\n// 1️⃣ The `summarize any(*) by EventType` operator groups the dataset by EventType\r\n//     and returns an arbitrary (non-deterministic) sample row for each EventType.\r\n//\r\n// 2️⃣ The `sort by EventType asc` operator sorts the resulting rows in ascending order by EventType.\r\n//\r\n// ✅ This is useful when you want to see example rows for each event type without retrieving all records.\r\ndatabase('Samples').StormEvents\r\n| summarize any(*) by EventType\r\n| sort by EventType asc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `count` operator in KQL\r\n\r\n### Purpose\r\nThe `count` operator is used to **quickly return the number of rows** in the input dataset.  \r\nIt provides a simple way to get a total row count without any grouping or additional aggregation logic.  \r\n\r\n### Behavior\r\nWhen used, `count` returns a single row containing a column named `Count` with the total number of rows.  \r\nIt ignores column values and only evaluates the number of rows.  \r\n\r\n### Difference from `summarize`\r\n- `count` is essentially a shorthand for `summarize count()`, but simpler and more concise.  \r\n- `summarize` allows grouping and multiple aggregations, while `count` provides a single, global count.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| count\r\n```  \r\n\r\n### Performance notes\r\n`count` is very efficient because it does not require materializing or transferring row data — it only computes the row count.  \r\nIt can be executed quickly even on large tables, as it leverages internal optimizations.  \r\n\r\n### Remarks\r\n- `count` always outputs a column named `Count`.  \r\n- It does not support additional columns or expressions.  \r\n- Commonly used to verify data volume, test filters, or validate query results.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query counts the total number of rows in the 'SalesFact' table from the 'ContosoSales' database\r\n// for a specified date range.\r\n//\r\n// 1️⃣ The `where` operator filters records to include only those with 'DateKey' between April 1, 2007, and May 31, 2007.\r\n//     - The range is inclusive and uses datetime literals for clear date boundaries.\r\n//\r\n// 2️⃣ The `count` operator returns a single row with a column named 'Count', showing the total number of matching records.\r\n//\r\n// ✅ This is useful when you want to quickly check how many sales transactions occurred during a specific time window,\r\n// for example, to monitor seasonal trends or campaign effectiveness.\r\ndatabase('ContosoSales').SalesFact\r\n| where DateKey between (datetime(\"2007-04-01\") .. datetime(\"2007-05-31\"))\r\n| count;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `join` operator in KQL\r\n\r\n### Purpose\r\nThe `join` operator is used to **combine rows from two tables** based on a common key.  \r\nIt allows you to enrich data, correlate events, or bring together related information from different datasets.  \r\n\r\n### Behavior\r\nYou specify a left table and a right table and define the keys (columns) on which to match.  \r\nBy default, KQL performs an inner join, but other join kinds (such as leftouter, rightouter, fullouter, innerunique, anti, and others) are supported.  \r\n\r\n### Join kinds\r\n- **inner**: Returns matching rows from both sides (default).  \r\n- **leftouter**: Returns all rows from the left side plus matching rows from the right side; unmatched right columns will be null.  \r\n- **rightouter**: Returns all rows from the right side plus matching rows from the left side.  \r\n- **fullouter**: Returns all rows from both sides, with nulls where no match exists.  \r\n- **innerunique**: Matches each row from the left with at most one row from the right.  \r\n- **anti**: Returns rows from the left side that have no matching rows in the right side.  \r\n- **semi**: Returns rows from the left side that have at least one match in the right side.  \r\n\r\n### Prefixing columns\r\nWhen columns with the same name exist in both tables, columns from the right table are prefixed with `right_` to avoid conflicts.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableA\r\n| join kind=inner TableB on CommonColumn\r\n```  \r\n\r\n### Performance notes\r\n- Joining large datasets can be resource-intensive.  \r\n- Use `project` or `project-keep` before the join to reduce the number of columns and improve efficiency.  \r\n- If possible, filter or summarize data before joining to reduce row count.  \r\n\r\n### Remarks\r\n- The join operator can significantly reshape and expand the data depending on the join type and keys used.  \r\n- The choice of join kind affects both the result set and performance.  \r\n- Always verify key columns and consider adding suffixes or prefixes explicitly to avoid ambiguity in result columns.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'Customers' table in the 'ContosoSales' database,\r\n// performs an inner join with the 'SalesFact' table on 'CustomerKey' (matching rows only),\r\n// note: the default join kind is 'innerunique', so explicitly specifying 'kind = inner' overrides this default and performs a standard inner join instead,\r\n// selects (projects) the columns 'CustomerKey', 'FirstName', and 'SalesAmount' from the joined result,\r\n// for best performance, if one table is always smaller, it should be placed on the left side of the join,\r\n// finally takes up to 20 rows.\r\n// The join algorithm optimizes by building a hash table of the left side (similar to a \"build side\" in traditional hash joins).\r\n// If the left side is smaller, it reduces memory usage and improves lookup speed when joining with the right side.\r\ndatabase(\"ContosoSales\").Customers\r\n| join kind = inner database(\"ContosoSales\").SalesFact on $left.CustomerKey == $right.CustomerKey\r\n| project CustomerKey, FirstName, SalesAmount\r\n| take 20;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Defines 'dsCustomers' as filtered customers with marital status \"M\" and selected columns.\r\n// Defines 'dsSalesFact' as summarized sales amounts per customer.\r\n// Performs a rightouter join on 'CustomerKey'.\r\n// After the join, Kusto automatically renames the right 'CustomerKey' to 'CustomerKey1' to avoid conflicts.\r\n// In 'project', explicit names are assigned using brackets for clarity and to distinguish left and right keys.\r\nlet dsCustomers = (\r\n    database(\"ContosoSales\").Customers\r\n    | where MaritalStatus == \"M\"\r\n    | project CustomerKey, FirstName, LastName\r\n    );\r\nlet dsSalesFact = database(\"ContosoSales\").SalesFact\r\n    | summarize TotalSalesAmount =sum(SalesAmount) by CustomerKey;    \r\ndsCustomers  \r\n| join kind = rightouter dsSalesFact on $left.CustomerKey == $right.CustomerKey\r\n| project\r\n    [\"ContosoSales.CustomerKey\"] = CustomerKey,\r\n    [\"SalesFact.CustomerKey\"] = CustomerKey1,\r\n    CustomerName = strcat(FirstName, \" \", LastName),\r\n    TotalSalesAmount\r\n| take 5\r\n| sort by ['ContosoSales.CustomerKey'];",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = innerunique\r\n// Performs an inner join where the left table ('t1') is deduplicated on the join key ('t1_col1') before joining\r\n// Only distinct keys from the left table are used, and each is matched to all corresponding rows from the right table ('t2')\r\n// Schema: Includes all columns from both tables by default; since the columns have different names ('t1_col1', 't2_col1'), no automatic renaming is needed\r\n// Rows: Each unique key from 't1' matched with every corresponding row in 't2'\r\n// Finally, sorts the result by 't1_col1' and 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 2, 2, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = innerunique t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc, t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "\r\n// kind = inner\r\n// Performs a standard inner join without deduplication on either side\r\n// All matching rows from the left table ('t1') and right table ('t2') are combined\r\n// If a key value occurs multiple times in both tables, all possible combinations (cross product per key) are included\r\n// Schema: Includes all columns from both tables by default; since columns are named differently ('t1_col1', 't2_col1'), no automatic renaming is needed\r\n// Rows: Contains every combination of rows where 't1_col1' equals 't2_col1'\r\n// Finally, sorts the result by 't1_col1' and 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = inner t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc, t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = leftouter\r\n// Performs a left outer join: all rows from the left table ('t1') are included, regardless of whether a match exists in the right table ('t2')\r\n// For matching keys, all combinations of rows from both tables are included (cross product per key)\r\n// For keys in the left table without a match in the right table, the columns from the right table are filled with nulls\r\n// Schema: Includes all columns from both tables by default; no automatic renaming needed here, as column names are distinct ('t1_col1', 't2_col1')\r\n// Rows: Contains all rows from 't1' and the matching rows from 't2'; if no match, right-side columns are null\r\n// Finally, sorts the result by 't1_col1' and 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = leftouter t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc, t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = rightouter\r\n// Performs a right outer join: all rows from the right table ('t2') are included, regardless of whether a match exists in the left table ('t1')\r\n// For matching keys, all combinations of rows from both tables are included (cross product per key)\r\n// For keys in the right table without a match in the left table, the columns from the left table are filled with nulls\r\n// Schema: Includes all columns from both tables by default; no automatic renaming needed here, as column names are distinct ('t1_col1', 't2_col1')\r\n// Rows: Contains all rows from 't2' and the matching rows from 't1'; if no match, left-side columns are null\r\n// Finally, sorts the result by 't1_col1' and 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = rightouter t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc, t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = fullouter\r\n// Performs a full outer join: includes all rows from both tables ('t1' and 't2'), regardless of whether a match exists\r\n// For matching keys, all combinations of rows from both tables are included (cross product per key)\r\n// For keys without a match in the other table, columns from the non-matching side are filled with nulls\r\n// Schema: Includes all columns from both tables by default; no automatic renaming needed here, as column names are distinct ('t1_col1', 't2_col1')\r\n// Rows: Contains all rows from both 't1' and 't2'; for non-matching keys, missing side columns are null\r\n// Finally, sorts the result by 't1_col1' and 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = fullouter t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc, t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = leftsemi\r\n// Performs a semi join: returns only rows from the left table ('t1') that have at least one matching row in the right table ('t2')\r\n// No columns from the right table ('t2') are included in the result\r\n// Schema: Includes only columns from the left table ('t1'); right-side columns are not present at all\r\n// Rows: Contains all rows from 't1' that have at least one match in 't2'; duplicates from 't1' are preserved if they exist in the input\r\n// Finally, sorts the result by 't1_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = leftsemi t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = leftantisemi\r\n// Performs an anti semi join: returns only rows from the left table ('t1') that have no matching rows in the right table ('t2')\r\n// No columns from the right table ('t2') are included in the result\r\n// Schema: Includes only columns from the left table ('t1'); right-side columns are not present at all\r\n// Rows: Contains all rows from 't1' for which there is no corresponding match in 't2'; duplicates from 't1' are preserved if they exist in the input\r\n// Finally, sorts the result by 't1_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = leftantisemi t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t1_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = rightsemi\r\n// Performs a semi join: returns only rows from the right table ('t2') that have at least one matching row in the left table ('t1')\r\n// No columns from the left table ('t1') are included in the result\r\n// Schema: Includes only columns from the right table ('t2'); left-side columns are not present at all\r\n// Rows: Contains all rows from 't2' that have at least one match in 't1'; duplicates from 't2' are preserved if they exist in the input\r\n// Finally, sorts the result by 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = rightsemi t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// kind = rightantisemi\r\n// Performs an anti semi join: returns only rows from the right table ('t2') that have no matching rows in the left table ('t1')\r\n// No columns from the left table ('t1') are included in the result\r\n// Schema: Includes only columns from the right table ('t2'); left-side columns are not present at all\r\n// Rows: Contains all rows from 't2' for which there is no corresponding match in 't1'; duplicates from 't2' are preserved if they exist in the input\r\n// Finally, sorts the result by 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nt1\r\n| join kind = rightantisemi t2 on $left.t1_col1 == $right.t2_col1\r\n| sort by t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Simulated cross join\r\n// KQL does not have a native cross join operator. However, you can simulate a cross join by adding a constant placeholder key to both tables and then joining on this key\r\n// In this example, both tables ('t1' and 't2') are extended with a new column 'placeholder' set to 1 for all rows\r\n// An inner join is then performed on this placeholder column, which effectively produces a Cartesian product: each row from 't1' is paired with every row from 't2'\r\n// Schema: After the join, all columns from both tables are available; here, only 't1_col1' and 't2_col1' are projected\r\n// Rows: Contains every possible combination of rows from 't1' and 't2'\r\n// Finally, sorts the result by 't1_col1' and 't2_col1' in ascending order\r\nlet t1 = datatable (t1_col1: int)\r\n[\r\n    1, 1, 3, 3, 5, 6, 7, 7, 9\r\n];\r\nlet t2 = datatable (t2_col1: int)\r\n[\r\n    2, 4, 5, 5, 6, 6, 7, 7, 8, 10\r\n];\r\nlet t1e = t1\r\n    | extend placeholder = 1;\r\nlet t2e = t2\r\n    | extend placeholder = 1;\r\nt1e\r\n| join kind = inner t2e on $left.placeholder == $right.placeholder\r\n| project t1_col1, t2_col1\r\n| sort by t1_col1 asc, t2_col1 asc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `union` operator in KQL\r\n\r\n### Purpose\r\nThe `union` operator is used to **combine rows from two or more tables** into a single result set.  \r\nIt is useful when you want to analyze or view data together from multiple sources with similar or identical schemas.  \r\n\r\n### Behavior\r\n`union` returns all rows from each input table.  \r\nThe resulting schema is a superset of all columns present in the input tables.  \r\nIf a column is missing from one table, it will be filled with null values for that table's rows.  \r\n\r\n### Duplicate rows\r\nBy default, `union` includes duplicate rows.  \r\nIf you want distinct rows, you can add the `distinct` operator after the union.  \r\n\r\n### Column alignment\r\nColumns are matched by name.  \r\nIf schemas differ, columns not present in some tables will contain nulls.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableA\r\n| union TableB\r\n```  \r\n\r\n### Performance notes\r\n- When combining large datasets, using `project` or `project-keep` before the union can improve performance by reducing columns.  \r\n- You can use filters on each individual table before the union to minimize the data volume.  \r\n\r\n### Remarks\r\n- The order of rows in the result set is not guaranteed after a union.  \r\n- You can union more than two tables by chaining multiple inputs.  \r\n- Useful for consolidating historical and live data, merging logs, or aggregating data from different sources.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'NewSales' and 'SalesTable' tables in the 'ContosoSales' database,\r\n// combines (unifies) all rows from both tables using the 'union' operator,\r\n// then counts the total number of rows in the combined result set,\r\n// returns a single row with a 'Count' column showing the total combined row count.\r\ndatabase('ContosoSales').NewSales\r\n| union\r\ndatabase('ContosoSales').SalesTable\r\n| count;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// The following query then unifies (unions) all tables in the 'ContosoSales' database whose names match the wildcard pattern '*Sales*',\r\n// combines all their rows into a single result set,\r\n// and finally counts the total number of rows across these tables,\r\n// returning a single row with a 'Count' column showing the overall combined row count.\r\nunion database('ContosoSales').*Sales*\r\n| count;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Defines 'NewSales' as a subset of the 'NewSales' table from the 'ContosoSales' database,\r\n// keeps only 'CustomerKey', 'SalesAmount', and 'CompanyName' columns,\r\n// filters rows with 'SalesAmount' greater than 1000, then takes the first 3 rows.\r\n//\r\n// Defines 'SalesTable' as a subset of the 'SalesTable' table from the 'ContosoSales' database,\r\n// keeps only 'CustomerKey', 'SalesAmount', and 'Country' columns,\r\n// filters rows with 'SalesAmount' greater than 500, then takes the first 3 rows.\r\n//\r\n// Combines (unifies) the rows from 'NewSales' and 'SalesTable' using the 'union' operator,\r\n// resulting in a single set that includes rows from both filtered tables.\r\nlet NewSales = database('ContosoSales').NewSales\r\n| project CustomerKey, SalesAmount, CompanyName\r\n| where SalesAmount > 1000\r\n| take 3;\r\nlet SalesTable = database('ContosoSales').SalesTable\r\n| project CustomerKey, SalesAmount, Country\r\n| where SalesAmount > 500\r\n| take 3;\r\nNewSales\r\n| union SalesTable;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the top 5 customer records from the 'NewSales' table in the 'ContosoSales' database,\r\n// and the top 5 customer records from the 'SalesTable' table in the same database,\r\n// then combines these two result sets into a single unified set using the 'union' operator.\r\n//\r\n// The 'withsource' argument is used with 'union' to add an extra column named 'Table',\r\n// which indicates the source table ('NewSales' or 'SalesTable') each row came from.\r\n//\r\n// This helps identify where each row originated when viewing or analyzing the combined data.\r\n(\r\n    database('ContosoSales').NewSales\r\n    | project CustomerKey, SalesAmount, CompanyName\r\n    | take 5\r\n)\r\n| union withsource=\"Table\" (\r\n    database('ContosoSales').SalesTable\r\n    | project CustomerKey, SalesAmount, City\r\n    | take 5\r\n);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the top 5 customer records from the 'NewSales' table in the 'ContosoSales' database,\r\n// and the top 5 customer records from the 'SalesTable' table in the same database,\r\n// then combines these two result sets into a single unified set using the 'union' operator.\r\n//\r\n// Here, the 'union' operator is written at the top level, making it clear that it unifies multiple table sources directly.\r\n//\r\n// The 'withsource' argument is used to add a new column named 'Table',\r\n// which indicates the source table ('NewSales' or 'SalesTable') each row came from.\r\n// This helps track the origin of each row when analyzing or displaying the combined data.\r\nunion withsource=\"Table\"\r\n(\r\n    database('ContosoSales').NewSales\r\n    | project CustomerKey, SalesAmount, CompanyName\r\n    | take 5\r\n),\r\n(\r\n    database('ContosoSales').SalesTable\r\n    | project CustomerKey, SalesAmount, City\r\n    | take 5\r\n);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the top 5 customer records from the 'NewSales' table in the 'ContosoSales' database,\r\n// and the top 5 customer records from the 'SalesTable' table in the same database,\r\n// then combines these two result sets into a single unified set using the 'union' operator.\r\n//\r\n// The 'union' operator is placed at the top level to clearly and efficiently combine multiple sources.\r\n//\r\n// The 'withsource' argument adds a new column named 'Table' that indicates the source table ('NewSales' or 'SalesTable') each row came from.\r\n//\r\n// The 'kind=outer' parameter (which is also the default) ensures that all rows from both input tables are included in the result set.\r\n// Columns that do not exist in one of the source tables will contain null values for those rows in the combined output.\r\nunion kind=outer withsource=\"Table\"\r\n(\r\n    database('ContosoSales').NewSales\r\n    | project CustomerKey, SalesAmount, CompanyName\r\n    | take 5\r\n),\r\n(\r\n    database('ContosoSales').SalesTable\r\n    | project CustomerKey, SalesAmount, City\r\n    | take 5\r\n);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the top 5 customer records from the 'NewSales' table in the 'ContosoSales' database,\r\n// and the top 5 customer records from the 'SalesTable' table in the same database,\r\n// then combines these two result sets into a single unified set using the 'union' operator.\r\n//\r\n// The 'union' operator is placed at the top level to clearly combine multiple sources.\r\n//\r\n// The 'withsource' argument adds a new column named 'Table' that indicates the source table ('NewSales' or 'SalesTable') each row came from.\r\n//\r\n// The 'kind=inner' parameter ensures that only columns that exist in both source tables are included in the result set.\r\n// Columns unique to one table are excluded from the final output.\r\nunion kind=inner withsource=\"Table\"\r\n(\r\n    database('ContosoSales').NewSales\r\n    | project CustomerKey, SalesAmount, CompanyName\r\n    | take 5\r\n),\r\n(\r\n    database('ContosoSales').SalesTable\r\n    | project CustomerKey, SalesAmount, City\r\n    | take 5\r\n);",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `range` operator in KQL\r\n\r\n### Purpose\r\nThe `range` operator is used to **generate a table of computed values** over a specified range.  \r\nIt is often used to create synthetic datasets, generate series of numbers or dates, and support test or demonstration scenarios.  \r\n\r\n### Behavior\r\nYou define a column name, a start value, an end value, and a step size.  \r\nA new row is created for each value in the generated sequence.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nrange ColumnName from StartValue to EndValue step StepValue\r\n```  \r\nThis creates a column named `ColumnName` with values starting from `StartValue` to `EndValue`, incremented by `StepValue`.  \r\n\r\n### Difference from `datatable`\r\n- `datatable` allows explicit definition of values in rows.  \r\n- `range` generates values automatically based on numeric or datetime increments.  \r\n\r\n### Supported types\r\n- Numeric types (int, long, real)  \r\n- Datetime values  \r\n\r\n### Performance notes\r\n`range` is highly efficient since it generates data in memory and does not scan or read from existing tables.  \r\nIt is optimized for generating large sequences quickly.  \r\n\r\n### Remarks\r\n- Useful for joining or correlating with real data to fill gaps (e.g., generating all possible time slots).  \r\n- Can be used together with `extend` to add calculated columns after generation.  \r\n- Helpful for creating scaffolding or time buckets in time series analyses.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Generates a table with a single column named 'myNumbers',\r\n// creates numeric values starting from 10 up to 100 (inclusive),\r\n// each subsequent value is incremented by 5,\r\n// the result is a sequence of rows with numbers: 10, 15, 20, ..., 100.\r\nrange myNumbers from 10 to 100 step 5;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Generates a table with a single column named 'myHours',\r\n// creates datetime values starting from '2025-01-01 00:00:00' to '2025-01-02 00:00:00' (inclusive),\r\n// each subsequent value is incremented by 1 hour,\r\n// the result is a sequence of rows representing each hour within that date range.\r\nrange myHours from todatetime('2025-01-01') to todatetime('2025-01-02') step 1h;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Generates a table with a single column named 'myDays',\r\n// creates datetime values starting from '2025-01-01' to '2025-02-28' (inclusive),\r\n// each subsequent value is incremented by 1 day,\r\n// the result is a sequence of rows representing each day within that date range.\r\nrange myDays from todatetime('2025-01-01') to todatetime('2025-02-28') step 1d;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Generates a table with a single column named 'monthIndex',\r\n// creates integer values from 1 to 12, incrementing by 1 (representing months),\r\n// then uses 'extend' to add a new column 'myMonth' which calculates a datetime value\r\n// by adding (monthIndex - 1) months to the base date '2025-01-01',\r\n// the result is a list of rows showing each month in 2025 starting from January.\r\nrange monthIndex from 1 to 12 step 1\r\n| extend myMonth = datetime_add(\"month\", monthIndex - 1, todatetime(\"2025-01-01\"));",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Creates two time ranges:\r\n// 'my_time_range_01' generates numbers from 0 to 10 in steps of 1 (full set),\r\n// 'my_time_range_02' generates numbers from 0 to 10 in steps of 2 (sampled subset),\r\n// performs a left anti semi join to find numbers that exist in 'my_time_range_01' but not in 'my_time_range_02',\r\n// this is useful for identifying gaps or missing values in the sampled time range.\r\nlet my_time_range_01 = (\r\n    range numbers from 0 to 10 step 1);\r\nlet my_time_range_02 = (\r\n    range numbers from 0 to 10 step 2);\r\nmy_time_range_01\r\n| join kind=leftantisemi my_time_range_02 on numbers;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `lookup` operator in KQL\r\n\r\n### Purpose\r\nThe `lookup` operator is used to **enrich rows from a main (left) table with data from another (right) table**, similar to a left outer join in SQL.  \r\nIt simplifies common join scenarios where you need to add additional information to an existing dataset without removing rows.  \r\n\r\n### Behavior\r\nEach row from the left table is matched with rows from the right table based on specified key columns.  \r\nIf there is no match, columns from the right table will be null.  \r\nUnlike `join kind=leftouter`, `lookup` has simplified syntax and defaults to keeping all rows from the left side.  \r\n\r\n### Difference from `join`\r\n- `lookup` is equivalent to a `leftouter` join but requires less explicit configuration.  \r\n- It automatically prefixes columns from the right side with `right_` when there are name conflicts.  \r\n- It is designed to make enrichment queries more concise and readable.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nLeftTable\r\n| lookup RightTable on KeyColumn\r\n```  \r\n\r\n### Performance notes\r\n`lookup` is generally efficient but can become resource-intensive if the right table is large or not pre-filtered.  \r\nFiltering or projecting columns in the right table before the `lookup` improves performance and reduces data transfer.  \r\n\r\n### Remarks\r\n- All rows from the left table are preserved.  \r\n- Columns from the right table are added; unmatched rows have nulls in these columns.  \r\n- Useful for adding descriptive or reference data (e.g., lookup tables with labels or metadata).  \r\n- Only supports equality matches on keys (no complex conditions).  \r\n\r\n### Additional performance note\r\nThe `lookup` operator is a special implementation of a join operator optimized for scenarios where a large (fact) table is enriched with data from a smaller (dimension) table.  \r\nIt extends the rows of the large table with values looked up from the smaller table.  \r\nFor best performance, the system assumes by default that the left table is the large (fact) table and the right table is the small (dimension) table.  \r\nThis assumption is the **opposite** of the assumption used by the standard `join` operator, which expects the smaller table on the left side for best performance.",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'SalesTable' table in the 'ContosoSales' database,\r\n// filters rows to include only those with 'DateKey' between July 1, 2023, and December 31, 2023,\r\n// then uses the 'lookup' operator to enrich each sales row with data from the 'Customers' table by matching 'CustomerKey',\r\n// the lookup extends each sales row with customer information, and if no match is found, customer columns will be null,\r\n// finally, selects (projects) the columns 'CustomerKey', 'FirstName', and 'SalesAmount' from the joined result.\r\n// Total CPU time: 00:00.062 (0,063 seconds)\r\n// Data scanned (estimated): 1,8 MB (1.892.266 bytes)\r\n// Peak memory: 56,6 MB (59.350.336 bytes)\r\ndatabase(\"ContosoSales\").SalesTable\r\n| where DateKey between (todatetime('2023-07-01') .. todatetime('2023-12-31'))\r\n| lookup database(\"ContosoSales\").Customers on $left.CustomerKey == $right.CustomerKey\r\n| project CustomerKey, FirstName, SalesAmount;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// join instead of lookup\r\n// Total CPU time: 00:00.093 (0,094 seconds)\r\n// Data scanned (estimated): 1,3 MB (1.357.914 bytes)\r\n// Peak memory: 16,1 MB (16.836.144 bytes)\r\nlet lSalesTable = database(\"ContosoSales\").SalesTable\r\n    | where DateKey between (todatetime('2023-07-01') .. todatetime('2023-12-31'));\r\ndatabase(\"ContosoSales\").Customers\r\n| join kind = inner lSalesTable on $left.CustomerKey == $right.CustomerKey\r\n| project CustomerKey, FirstName, SalesAmount;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'SalesFact' table in the 'ContosoSales' database,\r\n// uses the 'lookup' operator to enrich each sales row with product information from the 'Products' table by matching on 'ProductKey',\r\n// then summarizes the data by counting the number of sales rows for each 'ProductCategoryName',\r\n// calculates 'TotalSales' as the count of sales per category,\r\n// finally orders the result in descending order of 'TotalSales' to show the most popular product categories first.\r\n// Total CPU time: 00:00.203 (0,203 seconds)\r\n// Data scanned (estimated): 1,8 MB (1.888.704 bytes)\r\n// Peak memory: 6 MB (6.337.248 bytes)\r\ndatabase(\"ContosoSales\").SalesFact\r\n| lookup database(\"ContosoSales\").Products on ProductKey\r\n| summarize TotalSales = count() by ProductCategoryName\r\n| order by TotalSales desc;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// join instead of lookup\r\n// Total CPU time: 00:00.265 (0,266 seconds)\r\n// Data scanned (estimated): 1,8 MB (1.886.856 bytes)\r\n// Peak memory: 4,5 MB (4.674.352 bytes)\r\ndatabase(\"ContosoSales\").Products \r\n| join kind=rightouter database(\"ContosoSales\").SalesFact on ProductKey\r\n| summarize TotalSales = count() by ProductCategoryName\r\n| order by TotalSales desc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `mv-expand` operator in KQL\r\n\r\n### Purpose\r\nThe `mv-expand` operator is used to **expand dynamic arrays or property bags into multiple rows**, so each element of the collection becomes its own row.  \r\nIt is commonly used to normalize multi-value fields (e.g., arrays in JSON data) for easier analysis.  \r\n\r\n### Behavior\r\nWhen applied to a column containing dynamic arrays, `mv-expand` produces one row per element of the array, duplicating the other columns’ values for each new row.  \r\nIt can also be used on multiple columns at once.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| mv-expand ArrayColumn\r\n```  \r\nThis expands the `ArrayColumn` so each element becomes a separate row.  \r\n\r\n### Output columns\r\nAfter expansion, the resulting column will contain individual values instead of arrays.  \r\nAdditional columns can be created to capture array indices or original positions if needed.  \r\n\r\n### Options\r\n- `bag_unpack`: Can be used to extract key-value pairs from a property bag (dynamic object) as separate columns.  \r\n- `with_itemindex`: Adds an index column indicating the original position of each item in the array.  \r\n\r\n### Difference from `extend` or `project`\r\n- `extend` and `project` create new columns or modify existing columns but keep one row per input row.  \r\n- `mv-expand` **generates multiple rows** from each input row, depending on array size.  \r\n\r\n### Performance notes\r\n`mv-expand` can increase row count significantly, leading to higher memory and compute usage.  \r\nIt is recommended to filter or limit data before expanding large arrays.  \r\n\r\n### Remarks\r\n- Especially useful for working with semi-structured data such as JSON fields or telemetry data.  \r\n- Can be combined with `summarize` or `join` after expansion for further analysis.  \r\n- If the column is empty or null, the original row is dropped by default (unless explicitly handled).  ",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `make-series` operator in KQL\r\n\r\n### Purpose\r\nThe `make-series` operator is used to **create series (arrays) of aggregated values over a continuous range**, typically for time series analysis or charting.  \r\nIt allows transforming grouped data into a shape that is easier to plot or analyze as a continuous sequence.  \r\n\r\n### Behavior\r\nYou specify one or more aggregation functions and a range for the series (for example, over time intervals).  \r\nThe result is a table where each row represents a grouping key, and the series columns contain arrays of aggregated values for each bin.  \r\n\r\n### Syntax\r\n \r\n```kql\r\nTableName\r\n| make-series AggregationFunction() on TimelineColumn from Start to End step StepSize by GroupingColumns\r\n```  \r\nThis generates series over the specified timeline or numeric range.  \r\n\r\n### Grouping\r\nThe `by` clause groups data before creating the series.  \r\nIf no grouping is specified, all data is aggregated into a single series.  \r\n\r\n### Difference from `summarize`\r\n- `summarize` aggregates data into single values per group.  \r\n- `make-series` aggregates data into **arrays of values**, representing trends over time or other continuous domains.  \r\n\r\n### Fill options\r\n- Missing bins (intervals with no data) are filled with `null` by default.  \r\n- You can specify `default=` to define a custom fill value (e.g., 0).  \r\n\r\n### Performance notes\r\n`make-series` can be resource-intensive if the time range is large or bin size is very small.  \r\nFiltering data beforehand (for example using `where`) is recommended to improve efficiency.  \r\n\r\n### Remarks\r\n- Useful for generating data for charts and visualizations, including line and area charts.  \r\n- Often combined with `render` for plotting directly in dashboards.  \r\n- Works best with time series or numerical ranges where continuity is important.  ",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `let` statement in KQL\r\n\r\n### Purpose\r\nThe `let` statement is used to **define reusable variables or expressions**, which can simplify complex queries and improve readability.  \r\nIt allows you to assign a name to a scalar value, a table expression, or a subquery result.  \r\n\r\n### Behavior\r\nOnce defined, the `let` variable can be referenced in subsequent parts of the query.  \r\nVariables are immutable and scoped to the query in which they are defined.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nlet VariableName = Expression;\r\n```  \r\nThe variable can then be used like a named constant or subquery result in later statements.  \r\n\r\n### Types of assignments\r\n- **Scalar values**: For example, numeric or datetime constants.  \r\n- **Table expressions**: Store a filtered or pre-processed subset of a table.  \r\n- **Expressions**: Save reusable calculations or expressions for clarity.  \r\n\r\n### Difference from functions\r\n- `let` defines variables local to a single query.  \r\n- User-defined functions (UDFs) are reusable across multiple queries and defined at the database level.  \r\n\r\n### Performance notes\r\nUsing `let` does not inherently change performance, but it can help optimize queries by avoiding repeated expressions.  \r\nWhen used with heavy expressions or subqueries, it can reduce redundant computation.  \r\n\r\n### Remarks\r\n- `let` statements must be terminated with a semicolon (`;`).  \r\n- Multiple `let` statements can be defined in sequence.  \r\n- Improves maintainability and makes long or complex queries easier to follow.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Defines 'TopTotalDamageEventType' as a scalar value,\r\n// which is the EventType that has the highest total combined damage (DamageCrops + DamageProperty)\r\n// in the 'StormEvents' table from the 'Samples' database.\r\n//\r\n// The subquery first extends a new column 'TotalDamage' by summing 'DamageCrops' and 'DamageProperty' per row,\r\n// then summarizes total damage by 'EventType',\r\n// finally selects the top 1 EventType with the highest total damage using 'top 1 by sum_TotalDamage desc',\r\n// and uses 'toscalar' to extract just the EventType value as a single scalar.\r\n//\r\n// After that, queries the 'StormEvents' table again,\r\n// projects (selects) columns 'EventType' and 'TotalDamage' (here set as just 'DamageCrops' for demonstration),\r\n// but does not yet use the scalar value in a filter — it could be further used to filter for only the top damage EventType if needed.\r\nlet TopTotalDamageEventType = toscalar (\r\n    database(\"Samples\").StormEvents\r\n    | extend TotalDamage = DamageCrops + DamageProperty\r\n    | summarize sum(TotalDamage) by EventType\r\n    | top 1 by sum_TotalDamage desc\r\n);\r\ndatabase(\"Samples\").StormEvents\r\n| project EventType, TotalDamage = DamageCrops;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Defines 'TopTotalDamageEventType' as a table containing the single EventType that has the highest total combined damage (DamageCrops + DamageProperty),\r\n// in the 'StormEvents' table from the 'Samples' database.\r\n//\r\n// The subquery first extends a new column 'TotalDamage' as the sum of 'DamageCrops' and 'DamageProperty',\r\n// then summarizes total damage grouped by 'EventType',\r\n// then selects the top 1 EventType with the highest total damage using 'top 1 by sum_TotalDamage desc'.\r\n//\r\n// Then performs an 'innerunique' join between 'TopTotalDamageEventType' and the full 'StormEvents' table on 'EventType',\r\n// 'innerunique' ensures that the left side (TopTotalDamageEventType) is deduplicated before joining.\r\n//\r\n// Finally, projects columns 'EventType' and 'TotalDamage' (where 'TotalDamage' is set to 'DamageCrops' just for demonstration),\r\n// resulting in all StormEvents rows that match the top damage EventType.\r\nlet TopTotalDamageEventType = \r\n    database(\"Samples\").StormEvents\r\n    | extend TotalDamage = DamageCrops + DamageProperty\r\n    | summarize sum(TotalDamage) by EventType\r\n    | top 1 by sum_TotalDamage desc;\r\nTopTotalDamageEventType\r\n| join kind=innerunique database(\"Samples\").StormEvents on $left.EventType == $right.EventType\r\n| project EventType, TotalDamage = DamageCrops;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query demonstrates how to use the 'let' statement to define inline functions and calculate derived date values.\r\n//\r\n// - Defines three functions:\r\n//     - 'previousMonthFirstDay': computes the first day of the previous month based on a given date.\r\n//     - 'previousMonthSameDay': computes the same day number in the previous month based on a given date.\r\n//     - 'currentMonthFirstDay': computes the first day of the current month.\r\n//\r\n// - Assigns a specific datetime value (April 13, 2022) to the variable 'theDate'.\r\n//\r\n// - Uses 'print' to display the original date and the calculated values from the three functions:\r\n//     - Prints the original date ('theDate').\r\n//     - Prints the first day of the current month.\r\n//     - Prints the first day of the previous month.\r\n//     - Prints the same day in the previous month.\r\nlet previousMonthFirstDay = (theDate: datetime) { datetime_add('month', -1, startofmonth(theDate)) };\r\nlet previousMonthSameDay = (theDate: datetime) { datetime_add('month', -1, theDate) };\r\nlet currentMonthFirstDay = (theDate: datetime) { startofmonth(theDate) };\r\nlet theDate = todatetime(make_datetime(2022, 4, 13));\r\nprint theDate = theDate;\r\nprint currentMonthFirstDay = currentMonthFirstDay(theDate);\r\nprint previousMonthFirstDay = previousMonthFirstDay(theDate);\r\nprint previousMonthSameDay = previousMonthSameDay(theDate);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Defines an inline function named 'Percentage' that takes two real (floating point) numbers: 'portion' and 'total',\r\n// and returns the rounded percentage value calculated as (portion / total) * 100, rounded to two decimal places.\r\n//\r\n// Queries the 'StormEvents' table from the 'Samples' database,\r\n// creates a new column 'Damage' as the sum of 'DamageCrops' and 'DamageProperty'.\r\n//\r\n// Summarizes data by 'EventType':\r\n//   - 'TotalEvents': counts all events for each event type.\r\n//   - 'TotalDamagingEvents': counts events where 'Damage' is greater than 0 using 'countif'.\r\n//\r\n// Projects (selects) columns:\r\n//   - 'EventType'.\r\n//   - 'TotalDamagingEvents'.\r\n//   - 'TotalEvents'.\r\n//   - 'Percentage': computes the percentage of damaging events out of total events by calling the custom 'Percentage' function.\r\n//\r\n// Finally, sorts the results by 'EventType' in ascending order.\r\nlet Percentage = (portion: real, total: real) { round(100 * portion / total, 2) };\r\ndatabase(\"Samples\").StormEvents\r\n| extend Damage = DamageCrops + DamageProperty\r\n| summarize TotalEvents = count(), TotalDamagingEvents = countif(Damage > 0) by EventType\r\n| project\r\n    EventType,\r\n    TotalDamagingEvents,\r\n    TotalEvents,\r\n    Percentage = Percentage(TotalDamagingEvents, TotalEvents)\r\n| sort by EventType;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `render` operator in KQL\r\n\r\n### Purpose\r\nThe `render` operator is used to **visualize query results as charts or plots** directly within supported tools (such as Azure Data Explorer, Kusto Web UI, and dashboards).  \r\nIt helps transform tabular data into visual representations for easier analysis and presentation.  \r\n\r\n### Behavior\r\nWhen appended to a query, `render` specifies how the result set should be displayed (e.g., as a time chart, pie chart, column chart).  \r\nThe underlying data is not changed — only its presentation format is defined.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nTableName\r\n| render ChartType\r\n```  \r\nThis displays the data using the specified chart type.  \r\n\r\n### Chart types\r\nSupports various visualization types, including:  \r\n- `timechart`  \r\n- `barchart`  \r\n- `columnchart`  \r\n- `piechart`  \r\n- `scatterchart`  \r\n- `areachart`  \r\n- `anomalychart` (for highlighting anomalies)  \r\n\r\n### Options\r\nYou can specify optional parameters to control aspects like series grouping, legend position, and axis formatting.  \r\nSyntax example: `render timechart with (title=\"My Chart\", yaxis=log)`  \r\n\r\n### Performance notes\r\n`render` does not affect query performance because it only impacts the client-side visualization.  \r\nHowever, choosing appropriate aggregation and filtering before `render` is important to avoid overcrowded or misleading charts.  \r\n\r\n### Remarks\r\n- Only available in clients that support visual output (e.g., Azure Data Explorer Web UI).  \r\n- Does not change or store data in the backend; it is purely for display.  \r\n- Useful for building interactive dashboards and visual analyses quickly without exporting data to external tools.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes storm event data from the 'StormEvents' table in the 'Samples' database,\r\n// focusing on crop damage across different states.\r\n//\r\n// 1️⃣ The `summarize` operator groups the data by 'State' and calculates two metrics for each state:\r\n//     - 'TotalEvents': Total number of events (using `count()`).\r\n//     - 'EventsWithDamageToCrops': Number of events where 'DamageCrops' is greater than 0 (using `countif()`).\r\n//\r\n// 2️⃣ The `top` operator selects the top 10 states based on 'EventsWithDamageToCrops',\r\n//     sorting them in descending order (by default) so that states with the most crop-damaging events appear at the top.\r\n//\r\n// 3️⃣ The `render barchart` operator visualizes the results as a bar chart directly in the query output,\r\n//     making it easy to see which states experienced the most crop-damaging storm events.\r\n//\r\n// ✅ This approach is useful for highlighting regions most affected by crop damage,\r\n// supporting data-driven decisions for resource allocation or mitigation planning.\r\ndatabase('Samples').StormEvents\r\n| summarize\r\n    TotalEvents = count(),\r\n    EventsWithDamageToCrops = countif(DamageCrops > 0)\r\n    by State\r\n| top 10 by EventsWithDamageToCrops\r\n| render barchart;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes storm event data from the 'Samples' database.\r\n//\r\n// 1️⃣ The `extend` operator creates a new column 'Damage' by adding 'DamageProperty' and 'DamageCrops'.\r\n//\r\n// 2️⃣ The `summarize` operator groups data into 7-day bins using `bin(StartTime, 7d)` and calculates:\r\n//     - 'NumberOfEvents': Total number of events in each bin.\r\n//     - 'TotalDamage': Sum of damage values in each bin.\r\n//\r\n// 3️⃣ The `render columnchart` operator visualizes the summarized data as a column chart.\r\n//\r\n// ✅ This is useful for seeing weekly trends in event counts and total damage.\r\ndatabase('Samples').StormEvents\r\n| extend Damage = DamageProperty + DamageCrops\r\n| summarize NumberOfEvents = count(), TotalDamage = sum(Damage) by bin(StartTime, 7d)\r\n| render columnchart;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes storm event damage over time by event type.\r\n//\r\n// 1️⃣ The `extend` operator creates:\r\n//     - 'Damage' as the sum of 'DamageProperty' and 'DamageCrops'.\r\n//     - 'MonthStart' representing the first day of each month using `startofmonth()`.\r\n//\r\n// 2️⃣ The `summarize` operator computes the average damage (rounded) per EventType and MonthStart, but only for events with Damage > 0.\r\n//\r\n// 3️⃣ The `project` operator selects only relevant columns.\r\n//\r\n// 4️⃣ The `render columnchart` operator plots a stacked column chart:\r\n//     - Each column represents a month.\r\n//     - Segments represent different EventTypes.\r\n//     - Additional chart options: legend hidden and custom title.\r\n//\r\n// ✅ This helps compare average monthly damages across event types visually.\r\ndatabase('Samples').StormEvents\r\n| extend\r\n    Damage = DamageProperty + DamageCrops,\r\n    MonthStart = startofmonth(StartTime)\r\n| summarize AverageDamage = round(avgif(Damage, Damage > 0)) by EventType, MonthStart\r\n| project MonthStart, EventType, AverageDamage\r\n| render columnchart\r\n    with (\r\n    kind=stacked,\r\n    series=EventType,\r\n    legend=hidden,\r\n    title=\"Average Damage per Month and Event Type\"\r\n    );",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query focuses on \"Hail\" events in three states: TEXAS, NEBRASKA, and KANSAS.\r\n//\r\n// 1️⃣ The `where` operator filters to those states and 'Hail' event type.\r\n//\r\n// 2️⃣ The `summarize` operator groups events by state and day using `bin(StartTime, 1d)`.\r\n//\r\n// 3️⃣ The `render timechart` operator visualizes each state’s daily event count in separate panels (using `ysplit=panels`).\r\n//\r\n// ✅ This is useful for comparing hail event trends across multiple states day by day.\r\ndatabase('Samples').StormEvents\r\n| where State in (\"TEXAS\", \"NEBRASKA\", \"KANSAS\") and EventType == \"Hail\"\r\n| summarize count() by State, bin(StartTime, 1d)\r\n| render timechart with (ysplit=panels);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query visualizes spatial distribution of events on a map using geo-point binning.\r\n//\r\n// 1️⃣ The `project` operator selects 'BeginLon' and 'BeginLat' columns.\r\n//\r\n// 2️⃣ The `where` operator filters out rows where either coordinate is null.\r\n//\r\n// 3️⃣ The `summarize` operator groups events into S2 cells of level 6 using `geo_point_to_s2cell()` and counts the number of events per cell.\r\n//\r\n// 4️⃣ The `project` operator converts each S2 cell hash to a central point for mapping.\r\n//\r\n// 5️⃣ The `extend` operator adds a label column 'Events' with a constant value \"count\".\r\n//\r\n// 6️⃣ The `render piechart` operator displays results on a map (with option `kind=map`).\r\n//\r\n// ✅ This is useful for visualizing event density geographically.\r\ndatabase(\"samples\").StormEvents \r\n| project BeginLon, BeginLat\r\n| where isnotnull(BeginLat) and isnotnull(BeginLon)\r\n| summarize count_summary = count() by hash = geo_point_to_s2cell(BeginLon, BeginLat, 6)\r\n| project geo_s2cell_to_central_point(hash), count_summary\r\n| extend Events = \"count\"\r\n| render piechart with (kind = map);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes a time series from the 'demo_make_series2' table.\r\n//\r\n// 1️⃣ It defines a time range from January 5, 2017 to February 3, 2017 at 22:00, with a 2-hour step.\r\n//\r\n// 2️⃣ The `make-series` operator creates a time series 'num' by averaging 'num' values over the defined intervals,\r\n//     grouped by 'sid' (series identifier).\r\n//\r\n// 3️⃣ The `where` operator selects only the series with sid == 'TS1' for focused analysis.\r\n//\r\n// 4️⃣ The `extend` operator decomposes the time series using `series_decompose()`:\r\n//     - 'baseline', 'seasonal', 'trend', and 'residual' components are extracted.\r\n//\r\n// 5️⃣ The `render timechart` operator visualizes the decomposition in a time chart with a custom title.\r\n//\r\n// ✅ This is useful for understanding underlying patterns in web app traffic, including trends and seasonality.\r\nlet min_t = datetime(2017-01-05);\r\nlet max_t = datetime(2017-02-03 22:00);\r\nlet dt = 2h;\r\ndemo_make_series2\r\n| make-series num=avg(num) on TimeStamp from min_t to max_t step dt by sid \r\n| where sid == 'TS1'   // select a single time series for a cleaner visualization\r\n| extend (baseline, seasonal, trend, residual) = series_decompose(num, -1, 'linefit')  // decompose into components\r\n| render timechart with(title='Web app. traffic over a month, decomposition');",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `series_outliers()` function in KQL\r\n\r\n### Purpose\r\nThe `series_outliers()` function is used to **detect outliers (anomalous points) in a dynamic numerical array**, often representing a time series.  \r\nIt helps identify unusual values that deviate significantly from expected patterns in the data.\r\n\r\n### Behavior\r\nThe function processes an input dynamic array and returns a dynamic array of boolean values:  \r\n- `true` indicates an outlier at that specific index.  \r\n- `false` indicates a normal (non-outlier) value.  \r\n\r\nBy default, the detection uses a robust statistical approach based on Median Absolute Deviation (MAD), but other algorithms (e.g., z-score) can also be specified.\r\n\r\n### Syntax\r\n\r\n```kql\r\nseries_outliers(series [, method [, threshold]])\r\n```  \r\n- `series`: The input dynamic numerical array.  \r\n- `method`: Optional. Specifies the detection method. Default is `'mad'` (Median Absolute Deviation). Other options include `'zscore'`.  \r\n- `threshold`: Optional. Controls the sensitivity of detection (e.g., higher threshold means fewer points marked as outliers).\r\n\r\n### Performance notes\r\n- The function is optimized for analyzing large time series arrays but can be computationally intensive on very large or complex arrays.  \r\n- It is recommended to pre-aggregate or bin data before using `series_outliers()` on very high-resolution series.\r\n\r\n### Remarks\r\n- Typically used in combination with `make-series` to create time series arrays before applying the outlier detection.  \r\n- Supports detection of both high and low outliers depending on chosen method and threshold.  \r\n- Especially useful for monitoring scenarios, anomaly detection, and quality control.  \r\n- You can visualize the results using `extend` to add the boolean array and then `mv-expand` to plot individual points.",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes authentication events on the \"MAIL-SERVER01\" host for the year 2022,\r\n// focusing on identifying abnormal (outlier) activity based on distinct source IP counts.\r\n//\r\n// 1️⃣ The `let AuthenticationEvents` block:\r\n//     - Retrieves records from the 'AuthenticationEvents' table in the 'SecurityLogs' database.\r\n//     - Filters events for hostname \"MAIL-SERVER01\" and timestamps within 2022.\r\n//     - Uses `make-series` to create a daily time series of distinct source IP counts (dcount(src_ip)) per username.\r\n//\r\n// 2️⃣ The second `let x` block:\r\n//     - Extends each time series with an 'outliers' array using the `series_outliers()` function,\r\n//       which flags unusual spikes or drops in the daily distinct IP count.\r\n//     - Uses `mvexpand` to flatten the arrays so each row now represents a single day with its count and outlier flag.\r\n//\r\n// 3️⃣ Finally, it outputs the expanded dataset containing:\r\n//     - 'dcount_src_ip': distinct count of source IPs for that day.\r\n//     - 'timestamp': corresponding day.\r\n//     - 'outliers': boolean indicating whether that day's value is considered an outlier.\r\n//\r\n// ✅ This approach is useful for detecting suspicious login patterns,\r\n// such as unexpected surges in the number of unique source IPs accessing a user account.\r\nlet AuthenticationEvents = (\r\n    database('SecurityLogs').AuthenticationEvents\r\n    | where hostname == \"MAIL-SERVER01\" and todatetime(timestamp) between (todatetime('20220101') .. todatetime('20221231'))\r\n    | make-series dcount(src_ip) on todatetime(timestamp) in range(todatetime('20220101'), todatetime('20221231'), 1d) by username)\r\n    ;\r\nlet x =(\r\n    AuthenticationEvents\r\n    | extend outliers = series_outliers(dcount_src_ip)\r\n    | mvexpand dcount_src_ip, timestamp, outliers to typeof(double))\r\n    ;\r\nx;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `materialize()` function in KQL\r\n\r\n### Purpose\r\nThe `materialize()` function is used to **cache and reuse a subquery result within the same query**, preventing it from being recomputed each time it is referenced.  \r\nIt is helpful for improving performance and readability when the same expensive subquery needs to be used multiple times.  \r\n\r\n### Behavior\r\nWhen you wrap a subquery or expression in `materialize()`, Kusto evaluates it once, stores the intermediate result, and then reuses it wherever referenced.  \r\nWithout `materialize()`, each reference to the same subquery would cause it to be fully re-executed.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nmaterialize(Subquery)\r\n```  \r\nThe resulting materialized table can be used in subsequent operators (e.g., multiple joins, unions, filters).  \r\n\r\n### Performance notes\r\nUsing `materialize()` can reduce query execution time and resource consumption when working with large or complex intermediate results.  \r\nHowever, materializing very large results may increase memory usage.  \r\n\r\n### Remarks\r\n- Recommended when the same subquery logic is used more than once in a query to avoid redundant computations.  \r\n- Only materializes data within the scope of the query execution; it does not persist data beyond the query.  \r\n- Useful for simplifying query logic by avoiding repeated code and ensuring consistent intermediate states.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Defines 'taxi_snapshot' as a materialized subquery from the 'nyc_taxi' table in the 'Samples' database,\r\n// selects 'vendor_id', 'pickup_datetime', 'total_amount', and 'tip_amount' columns,\r\n// filters rows where 'pickup_datetime' is on December 1, 2014,\r\n// and uses 'materialize()' to cache this filtered and projected snapshot for reuse multiple times in the query.\r\n//\r\n// Defines 'sum_of_total_amount' as a scalar value computed from 'taxi_snapshot',\r\n// representing the total sum of 'total_amount' for that day.\r\n//\r\n// Defines 'sum_of_tip_amount' as a scalar value computed from 'taxi_snapshot',\r\n// representing the total sum of 'tip_amount' for that day.\r\n//\r\n// Finally, processes 'taxi_snapshot' again,\r\n// and adds two new columns using 'extend':\r\n// - 'percentage_total_amount': shows each row's 'total_amount' as a percentage of the total sum, rounded to 2 decimal places.\r\n// - 'percentage_total_tip': shows each row's 'tip_amount' as a percentage of the total tip sum, rounded to 2 decimal places.\r\n//\r\n// Using 'materialize()' here avoids scanning and filtering the same subquery twice, improving performance and consistency.\r\nlet taxi_snapshot = materialize( \r\n    database('Samples').nyc_taxi \r\n    | project vendor_id, pickup_datetime, total_amount, tip_amount\r\n    | where pickup_datetime between (todatetime('2014-12-01') .. todatetime('2014-12-01'))\r\n);\r\nlet sum_of_total_amount = toscalar(\r\n    taxi_snapshot\r\n    | summarize sum(total_amount)\r\n);\r\nlet sum_of_tip_amount = toscalar(\r\n    taxi_snapshot\r\n    | summarize sum(tip_amount)\r\n);\r\ntaxi_snapshot\r\n| extend\r\n    percentage_total_amount = strcat(round((100 / sum_of_total_amount * total_amount), 2), \"%\"),\r\n    percentage_total_tip = strcat(round((100 / sum_of_tip_amount * tip_amount), 2), \"%\");",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `row_cumsum()` function in KQL\r\n\r\n### Purpose\r\nThe `row_cumsum()` function is used to **compute a running total (cumulative sum) over rows**, typically after a `sort` operator.  \r\nIt is useful for tracking progressive totals, such as cumulative sales, running counts, or incremental aggregations over time or ordered data.  \r\n\r\n### Behavior\r\n`row_cumsum()` takes a numeric expression as an argument and returns, for each row, the sum of that expression for all preceding rows including the current row.  \r\nThe order of the rows affects the result, so it is usually combined with `sort` to ensure consistent cumulative calculation.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nrow_cumsum(Expression)\r\n```  \r\nThe expression must evaluate to a numeric type (e.g., int, long, real).  \r\n\r\n### Performance notes\r\n`row_cumsum()` is evaluated row by row after sorting, which may affect performance on large datasets.  \r\nSorting before using `row_cumsum()` ensures predictable and correct results.  \r\n\r\n### Remarks\r\n- Very helpful for building running totals in reports or charts (e.g., progressive revenue or cumulative incident counts).  \r\n- Must be aware of the row order; always explicitly sort rows when necessary to guarantee correct cumulative logic.  \r\n- Unlike `summarize`, it does not group rows but computes row-wise totals in sequence.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'NewSales' table in the 'ContosoSales' database,\r\n// selects (projects) columns 'DateKey' and 'SalesAmount',\r\n// takes the first 100 rows from the result,\r\n// then sorts these rows in ascending order by 'DateKey' to ensure chronological ordering,\r\n// adds a new column 'runningSalesAmount' using 'row_cumsum(SalesAmount)',\r\n// which computes a running total of 'SalesAmount' across the sorted rows,\r\n// so each row shows the cumulative sales amount up to that point in time.\r\ndatabase('ContosoSales').NewSales\r\n| project DateKey, SalesAmount\r\n| take 100\r\n| sort by DateKey asc\r\n| extend runningSalesAmount = row_cumsum(SalesAmount);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'NewSales' table in the 'ContosoSales' database,\r\n// selects (projects) columns 'DateKey', 'CustomerKey', and 'SalesAmount',\r\n// filters rows to include only those where 'DateKey' is between May 22, 2022, and May 23, 2022,\r\n// then aggregates (summarizes) total sales amount per 'CustomerKey' and 'DateKey' by summing 'SalesAmount',\r\n// uses 'serialize' to assign a stable order to the rows based on 'CustomerKey' and 'DateKey' (important for consistent row-wise operations),\r\n// adds a new column 'runningSalesAmount' using 'row_cumsum(TotalSalesAmount)',\r\n// which computes a running total of 'TotalSalesAmount' across the serialized rows,\r\n// so each row shows the cumulative total sales amount up to that point for all customers and dates in order.\r\ndatabase('ContosoSales').NewSales\r\n| project DateKey, CustomerKey, SalesAmount\r\n| where DateKey between (todatetime(\"2022-05-22\") .. todatetime(\"2022-05-23\"))\r\n| summarize TotalSalesAmount = sum(SalesAmount) by CustomerKey, DateKey\r\n| serialize CustomerKey, DateKey\r\n| extend runningSalesAmount = row_cumsum(TotalSalesAmount);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves sales data from 'ContosoSales.NewSales' for the date range between May 22, 2022, and May 23, 2022,\r\n// projects (selects) columns 'DateKey', 'CustomerKey', and 'SalesAmount',\r\n// then sorts the data by 'CustomerKey' and 'DateKey' in ascending order.\r\n//\r\n// The 'row_cumsum' function computes a running total of 'SalesAmount'.\r\n// The cumulative sum resets whenever 'CustomerKey' changes — controlled by the condition 'CustomerKey != prev(CustomerKey)',\r\n// which evaluates to true when the current customer is different from the previous row's customer.\r\n//\r\n// In this query, the 'sort' operator actively reorders the rows based on 'CustomerKey' and 'DateKey',\r\n// ensuring that the running total is computed in the correct chronological order per customer.\r\n// This contrasts with the 'serialize' operator, which does not reorder rows but preserves their current order for row-wise calculations.\r\n//\r\n// The final output shows each row's sales amount and its corresponding per-customer running total.\r\ndatabase('ContosoSales').NewSales\r\n| project DateKey, CustomerKey, SalesAmount\r\n| where DateKey between (todatetime(\"2022-05-22\") .. todatetime(\"2022-05-23\"))\r\n| sort by CustomerKey asc, DateKey asc\r\n| extend runningSalesAmount = row_cumsum(SalesAmount, CustomerKey != prev(CustomerKey));",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves sales data from the 'ContosoSales.NewSales' table for the date range between May 22, 2022, and May 23, 2022.\r\n// It selects (projects) the columns 'DateKey', 'CustomerKey', and 'SalesAmount'.\r\n//\r\n// The data is sorted by 'CustomerKey' and 'DateKey' in ascending order to ensure correct row-by-row cumulative calculations.\r\n//\r\n// A constant field named 'Counter' is then added and set to 1 for every row, which serves as a base value for counting rows.\r\n//\r\n// The query then computes 'runningCounter' using 'row_cumsum(Counter, CustomerKey != prev(CustomerKey))'.\r\n// This running count resets whenever the 'CustomerKey' changes (i.e., when the current 'CustomerKey' is different from the previous row's customer).\r\n//\r\n// Similarly, it computes 'runningSalesAmount' using 'row_cumsum(SalesAmount, CustomerKey != prev(CustomerKey))',\r\n// which gives a running total of 'SalesAmount' for each customer, also resetting when the customer changes.\r\n//\r\n// Finally, the results are sorted again by 'CustomerKey', 'DateKey', and 'runningCounter' to maintain a clear, correct sequence in the final output.\r\n//\r\n// Note: The 'sort' operator explicitly reorders rows to guarantee that cumulative functions like 'row_cumsum' work as intended.\r\n// In contrast, the 'serialize' operator preserves the existing order without actually reordering the data.\r\ndatabase('ContosoSales').NewSales\r\n| project DateKey, CustomerKey, SalesAmount\r\n| where DateKey between (todatetime(\"2022-05-22\") .. todatetime(\"2022-05-23\"))\r\n| sort by CustomerKey asc, DateKey asc\r\n| extend Counter = 1 \r\n| extend runningCounter = row_cumsum(Counter, CustomerKey != prev(CustomerKey))\r\n| extend runningSalesAmount = row_cumsum(SalesAmount, CustomerKey != prev(CustomerKey))\r\n| sort by CustomerKey asc, DateKey asc, runningCounter asc;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `iff()` function in KQL\r\n\r\n### Purpose\r\nThe `iff()` function is used to **evaluate a condition and return one of two specified values**, similar to the ternary operator or `IF` statement in other languages.  \r\nIt is commonly used to introduce conditional logic directly inside projections, calculated columns, or expressions.  \r\n\r\n### Behavior\r\n`iff()` takes three arguments: a condition (boolean expression), a value to return if the condition is true, and a value to return if the condition is false.  \r\nThe function evaluates the condition for each row and returns the appropriate value accordingly.  \r\n\r\n### Syntax\r\n\r\n```kql\r\niff(condition, valueIfTrue, valueIfFalse)\r\n```  \r\n\r\n### Performance notes\r\n`iff()` is lightweight and evaluated row by row. It is optimized for use in large datasets and has minimal performance impact.  \r\n\r\n### Remarks\r\n- Useful for creating conditional columns or flags without writing verbose case logic.  \r\n- Can be nested to handle multiple conditions.  \r\n- The return values must be of compatible types; otherwise, a type conversion error will occur.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'SalesFact' table from the 'ContosoSales' database,\r\n// selects (projects) the columns 'TotalCost', 'SalesAmount', and 'ProductKey'.\r\n//\r\n// Then uses 'extend' to create a new column 'MarginPercentage',\r\n// which calculates the profit margin as a percentage: (SalesAmount - TotalCost) divided by SalesAmount, multiplied by 100.\r\n//\r\n// Next, adds another column 'Category' using the 'iif()' function:\r\n// - If 'MarginPercentage' is greater than 50, sets 'Category' to \"Category A\".\r\n// - Otherwise, sets 'Category' to \"Category B\".\r\n//\r\n// Finally, takes the first 100 rows from the result set.\r\ndatabase('ContosoSales').SalesFact\r\n| project TotalCost, SalesAmount, ProductKey\r\n| extend MarginPercentage = 100 / SalesAmount * (SalesAmount - TotalCost)\r\n| extend Category = iif(MarginPercentage > 50, \"Category A\", \"Category B\") \r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'SalesFact' table from the 'ContosoSales' database,\r\n// selects (projects) the columns 'TotalCost', 'SalesAmount', and 'ProductKey'.\r\n//\r\n// Then uses 'extend' to create a new column 'MarginPercentage',\r\n// which calculates the profit margin as a percentage: (SalesAmount - TotalCost) divided by SalesAmount, multiplied by 100.\r\n//\r\n// Adds another column 'CategoryMeasure', which is a boolean value:\r\n// - True if 'MarginPercentage' is greater than 50, otherwise False.\r\n//\r\n// Finally, creates a column 'Category' using the 'iif()' function:\r\n// - If 'CategoryMeasure' is true, sets 'Category' to \"Category A\".\r\n// - Otherwise, sets 'Category' to \"Category B\".\r\ndatabase('ContosoSales').SalesFact\r\n| project TotalCost, SalesAmount, ProductKey\r\n| extend MarginPercentage = 100 / SalesAmount * (SalesAmount - TotalCost)\r\n| extend CategoryMeasure = MarginPercentage > 50\r\n| extend Category = iif(CategoryMeasure, \"Category A\", \"Category B\");",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'SalesFact' table from the 'ContosoSales' database,\r\n// selects (projects) the columns 'TotalCost', 'SalesAmount', and 'ProductKey'.\r\n//\r\n// Then uses 'extend' to create a new column 'MarginPercentage',\r\n// which calculates the profit margin as a percentage: (SalesAmount - TotalCost) divided by SalesAmount, multiplied by 100.\r\n//\r\n// Adds another column 'Category' using nested 'iif()' logic to classify rows based on 'MarginPercentage':\r\n// - If 'MarginPercentage' is between 0 and 50 (inclusive), assigns \"Category A\".\r\n// - Otherwise, if 'MarginPercentage' is greater than 50 and less than 60, assigns \"Category B\".\r\n// - In all other cases (i.e., >= 60), assigns \"Category C\".\r\n//\r\n// Finally, takes the first 10 rows from the result set.\r\ndatabase('ContosoSales').SalesFact\r\n| project TotalCost, SalesAmount, ProductKey\r\n| extend MarginPercentage = 100 / SalesAmount * (SalesAmount - TotalCost)\r\n| extend Category = \r\n    iif(\r\n        MarginPercentage between (0 .. 50),\r\n        \"Category A\", \r\n        iif(\r\n            MarginPercentage > 50 and MarginPercentage < 60,\r\n            \"Category B\", \r\n            \"Category C\"\r\n        )\r\n    )\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `case()` function in KQL\r\n\r\n### Purpose\r\nThe `case()` function is used to **evaluate multiple conditions and return a corresponding value for the first true condition**, similar to a multi-branch IF or CASE statement in SQL.  \r\nIt is useful for implementing complex conditional logic in a concise and readable way within queries.  \r\n\r\n### Behavior\r\n`case()` takes an ordered list of condition-value pairs, and optionally a default value.  \r\nThe function evaluates each condition in order and returns the value corresponding to the first condition that evaluates to true.  \r\nIf none of the conditions are true and a default value is provided, it returns the default value; otherwise, it returns null.  \r\n\r\n### Syntax\r\n\r\n```kql\r\ncase(condition1, value1, condition2, value2, ..., defaultValue)\r\n```  \r\n\r\n### Performance notes\r\n`case()` is evaluated row by row and is optimized to stop checking once a true condition is found, minimizing computation overhead.  \r\n\r\n### Remarks\r\n- Conditions are checked in order, so the sequence matters.  \r\n- It is generally more readable and more concise than deeply nested `iif()` statements.  \r\n- The return values should be of the same or compatible types to avoid type conversion errors.  \r\n- When no default value is provided, and no condition matches, the result is null.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Generates a table using 'range' with a single column named 'Size',\r\n// creates integer values starting from 1 to 15, incrementing by 2,\r\n// resulting in the sequence: 1, 3, 5, 7, 9, 11, 13, 15.\r\n//\r\n// Then uses 'extend' to add a new column named 'bucket' using the 'case()' function,\r\n// which classifies each 'Size' value into categories:\r\n// - If 'Size' is less than or equal to 3, assigns \"Small\".\r\n// - Else if 'Size' is less than or equal to 10, assigns \"Medium\".\r\n// - Otherwise, assigns \"Large\".\r\nrange Size from 1 to 15 step 2\r\n| extend bucket = case(Size <= 3, \"Small\", \r\n                       Size <= 10, \"Medium\", \r\n                       \"Large\");",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// Queries the 'SalesFact' table from the 'ContosoSales' database,\r\n// selects (projects) the columns 'TotalCost', 'SalesAmount', and 'ProductKey'.\r\n//\r\n// Then uses 'extend' to create a new column 'MarginPercentage',\r\n// which calculates the profit margin as a percentage: (SalesAmount - TotalCost) divided by SalesAmount, multiplied by 100.\r\n//\r\n// Next, uses the 'case()' function to create a new column that categorizes rows based on 'MarginPercentage':\r\n// - If 'MarginPercentage' is between 0 and 50 (inclusive), assigns \"Category A\".\r\n// - If 'MarginPercentage' is greater than 50 and less than 60, assigns \"Category B\".\r\n// - Otherwise (default case), assigns \"Category C\".\r\n//\r\n// Finally, takes the first 20 rows from the result set.\r\ndatabase('ContosoSales').SalesFact\r\n| project TotalCost, SalesAmount, ProductKey\r\n| extend MarginPercentage = 100 / SalesAmount * (SalesAmount - TotalCost)\r\n| extend case(\r\n    MarginPercentage between (0 .. 50), \"Category A\",\r\n    MarginPercentage > 50 and MarginPercentage < 60, \"Category B\",\r\n    \"Category C\"\r\n)\r\n| take 20;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `distinct` operator in KQL\r\n\r\n### Purpose\r\nThe `distinct` operator is used to **return a unique set of rows based on specified columns**, effectively removing duplicates.  \r\nIt is similar to using `summarize by` but is simpler and more concise when no aggregation is needed.\r\n\r\n### Behavior\r\nWhen you use `distinct`, the query engine scans the data and returns one row for each unique combination of values in the specified columns.  \r\nColumns not included in the `distinct` clause are excluded from the result.\r\n\r\n### Syntax\r\n\r\n```kql\r\nT | distinct Column1, Column2, ...\r\n```  \r\n- `T`: The input table or dataset.  \r\n- `Column1, Column2, ...`: The columns used to determine uniqueness.\r\n\r\n### Performance notes\r\nThe `distinct` operator is generally efficient, especially on indexed or small column sets.  \r\nHowever, on very large datasets, distinct operations can require more memory and processing time because all unique value combinations must be identified.\r\n\r\n### Remarks\r\n- Equivalent to using `summarize by Column1, Column2, ...` when no aggregation function is needed.  \r\n- Useful for quickly exploring or listing all unique values in one or more columns.  \r\n- Can be combined with other operators, such as `where`, to pre-filter rows before applying distinct.  \r\n- The resulting output will only include the specified columns.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes data from the 'Dates' table in the 'ContosoSales' database,\r\n// focusing on retrieving unique combinations of year and half-year information.\r\n//\r\n// 1️⃣ The `where` operator filters the rows to include only those where 'CalendarYear' is between 2009 and 2011 (inclusive).\r\n//\r\n// 2️⃣ The `distinct` operator returns a unique set of rows based on 'CalendarYear' and 'CalendarHalfYear' columns:\r\n//     - Only one row is returned for each combination of these two columns.\r\n//     - Duplicate combinations are removed.\r\n//\r\n// 3️⃣ The `sort by` operator orders the resulting rows first by 'CalendarYear' in ascending order,\r\n//     and then by 'CalendarHalfYear' in ascending order.\r\n//\r\n// ✅ This approach is useful when you want to identify which year and half-year periods are represented in your data,\r\n// for example, to support building filters or summary dashboards.\r\ndatabase('ContosoSales').Dates\r\n| where CalendarYear between (2009 .. 2011)\r\n| distinct CalendarYear, CalendarHalfYear\r\n| sort by CalendarYear, CalendarHalfYear;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `prev()` function in KQL\r\n\r\n### Purpose\r\nThe `prev()` function is used to **access a value from a previous row in the query result set**, relative to the current row.  \r\nIt enables performing calculations that rely on row-to-row comparisons, such as differences, moving averages, or detecting changes in values.  \r\n\r\n### Behavior\r\n`prev()` returns the value of a specified expression from a preceding row, with an optional offset parameter indicating how many rows back to look (default is 1).  \r\nIf the offset goes beyond the first row, `prev()` returns null for that row.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nprev(expression [, offset])\r\n```  \r\n- `expression`: The column or calculated expression to fetch from a previous row.  \r\n- `offset`: (Optional) Number of rows back to retrieve the value. Default is 1.\r\n\r\n### Performance notes\r\n`prev()` operates after ordering rows. It requires an explicit or implicit order, typically ensured using `sort` or `serialize`, to guarantee correct row sequence.  \r\nIt is evaluated per row and has negligible performance impact when used properly.\r\n\r\n### Remarks\r\n- Commonly combined with functions like `row_cumsum()`, `iif()`, or arithmetic operations to implement advanced analytics logic (e.g., period-over-period changes).  \r\n- Can help detect resets or changes in groups when used with group-based sorting.  \r\n- Important: `prev()` depends on the ordering of rows; incorrect or missing sorting can lead to unexpected results.  \r\n- Works within the scope of a partition if a partitioning logic is applied (e.g., when using `row_number()` or `rank()`).  ",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `next()` function in KQL\r\n\r\n### Purpose\r\nThe `next()` function is used to **access a value from a following (next) row in the query result set**, relative to the current row.  \r\nIt enables performing forward-looking calculations, such as computing differences with the next value or detecting future state changes.\r\n\r\n### Behavior\r\n`next()` returns the value of a specified expression from a subsequent row, with an optional offset parameter indicating how many rows forward to look (default is 1).  \r\nIf the offset goes beyond the last row, `next()` returns null for that row.\r\n\r\n### Syntax\r\n\r\n```kql\r\nnext(expression [, offset])\r\n```  \r\n- `expression`: The column or calculated expression to fetch from a future row.  \r\n- `offset`: (Optional) Number of rows forward to retrieve the value. Default is 1.\r\n\r\n### Performance notes\r\n`next()` requires rows to be ordered explicitly using `sort` or `serialize` to ensure the correct sequence of evaluation.  \r\nIt is evaluated per row and generally does not introduce significant performance overhead when used appropriately.\r\n\r\n### Remarks\r\n- Useful for calculating differences between the current row and the next row (e.g., trend analysis, future state evaluation).  \r\n- Works well in combination with `prev()`, `iif()`, or arithmetic expressions to build advanced analytical columns.  \r\n- Proper sorting is critical; incorrect or missing sorting may produce unexpected results.  \r\n- Can be used within partitions when partitioning logic is defined (e.g., using `row_number()` or `rank()`).  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query demonstrates how to work with row order-dependent functions like `prev()` and `next()` in KQL.\r\n//\r\n// 1️⃣ It retrieves data from the 'NewSales' table in the 'ContosoSales' database,\r\n//     projecting only 'DateKey' and 'SalesAmount' columns.\r\n//\r\n// 2️⃣ The 'take 50' operator limits the dataset to the first 50 rows,\r\n//     reducing the volume for demonstration or testing purposes.\r\n//\r\n// 3️⃣ The 'serialize DateKey' operator forces a deterministic ordering of the rows based on 'DateKey'.\r\n//     This is critical because functions like 'prev()' and 'next()' depend on a consistent and defined row order.\r\n//     Without 'serialize', the engine may return rows in an arbitrary order, resulting in incorrect or unpredictable calculations.\r\n//\r\n// 4️⃣ The query then uses 'extend' to add two new columns:\r\n//     - 'previousSalesAmount' uses 'prev(SalesAmount, 1, 0)' to retrieve the SalesAmount from the previous row,\r\n//       defaulting to 0 if there is no previous row (e.g., for the first row).\r\n//     - 'nextSalesAmount' uses 'next(SalesAmount, 1, 0)' to retrieve the SalesAmount from the next row,\r\n//       defaulting to 0 if there is no next row (e.g., for the last row).\r\n//\r\n// These techniques are useful for performing row-by-row comparisons, detecting changes, or calculating differences between adjacent rows.\r\ndatabase('ContosoSales').NewSales\r\n| project DateKey, SalesAmount\r\n| take 50\r\n| serialize DateKey\r\n| extend previousSalesAmount = prev(SalesAmount, 1, 0)\r\n| extend nextSalesAmount = next(SalesAmount, 1, 0);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query demonstrates how explicit ordering affects row-based functions like `prev()` and `next()` in KQL.\r\n//\r\n// 1️⃣ It retrieves data from the 'NewSales' table in the 'ContosoSales' database,\r\n//     projecting only the 'DateKey' and 'SalesAmount' columns.\r\n//\r\n// 2️⃣ The 'take 50' operator limits the dataset to the first 50 rows,\r\n//     useful for restricting results during testing or analysis.\r\n//\r\n// 3️⃣ The 'sort by DateKey asc' operator explicitly sorts the rows in ascending order by 'DateKey'.\r\n//     This guarantees that row-based functions will operate over a known, ordered sequence.\r\n//\r\n// 4️⃣ The query then uses 'extend' to calculate:\r\n//     - 'previousSalesAmount' with 'prev(SalesAmount, 1, 0)', which retrieves the SalesAmount from the previous row,\r\n//       defaulting to 0 if there is no previous row (e.g., for the first row).\r\n//     - 'nextSalesAmount' with 'next(SalesAmount, 1, 0)', which retrieves the SalesAmount from the next row,\r\n//       defaulting to 0 if there is no next row (e.g., for the last row).\r\n//\r\n// 🔎 Difference between 'serialize' and 'sort by':\r\n// - 'sort by' explicitly reorders the rows based on specified column(s), in this case by 'DateKey' ascending.\r\n// - 'serialize' does not change the order but instructs the query engine to **preserve the current row order** \r\n//   when evaluating row-based functions.\r\n//\r\n// In this example, since 'sort by' is used, the order is explicitly defined,\r\n// and row-based functions operate correctly on the sorted sequence.\r\n// If you want to use the existing order without sorting, 'serialize' ensures that the engine respects that original order.\r\n//\r\n// This approach is useful when row-dependent calculations, such as lag/lead analysis or cumulative differences,\r\n// need to be based on a specific sorted sequence.\r\ndatabase('ContosoSales').NewSales\r\n| project DateKey, SalesAmount\r\n| take 50\r\n| sort by DateKey asc\r\n| extend previousSalesAmount = prev(SalesAmount, 1, 0)\r\n| extend nextSalesAmount = next(SalesAmount, 1, 0);",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `series_stats()` function in KQL\r\n\r\n### Purpose\r\nThe `series_stats()` function is used to **compute summary statistics for dynamic numerical arrays (series)** within a column.  \r\nIt is typically used in time series and sensor data scenarios to analyze individual series stored as dynamic arrays.  \r\n\r\n### Behavior\r\n`series_stats()` takes a dynamic array (series) as input and returns a dynamic object containing several summary metrics.  \r\nThese metrics include count, min, max, average (mean), variance, standard deviation, range, and the first and last elements.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nseries_stats(series)\r\n```  \r\n\r\n### Performance notes\r\n`series_stats()` is evaluated row by row and is optimized for working with dynamic columns containing array data.  \r\nIt enables in-query descriptive analysis without needing to unroll or expand arrays beforehand.  \r\n\r\n### Remarks\r\n- Returns a dynamic object that can be accessed using dot notation (e.g., `stats.mean`, `stats.min`).  \r\n- Useful when you have per-entity time series data stored in a single dynamic column (for example, sensor measurements or daily sales).  \r\n- Helps simplify logic for generating quick descriptive analytics on time series without additional transformation steps.  \r\n- If the input array is empty, all returned statistics will be null.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This script demonstrates how to create a dynamic array and analyze it using the `series_stats()` function.\r\n//\r\n// 1️⃣ It defines a dynamic array named 'myvalues' using the `dynamic()` function.\r\n//     The array contains the numeric values: [2, 6, 1, 9, 2, 5, 7, 1, 0, 9].\r\n//\r\n// 2️⃣ The `series_stats()` function is then applied to this array to compute summary statistics, including:\r\n//     - Count\r\n//     - Minimum\r\n//     - Maximum\r\n//     - Average (mean)\r\n//     - Variance\r\n//     - Standard deviation\r\n//     - Range\r\n//     - First and last values\r\n//\r\n// 3️⃣ Finally, the results are displayed using `print`, making it easy to review the computed statistics in a single row output.\r\n//\r\n// This approach is useful for quickly analyzing small sets of numeric data or dynamic series directly within a KQL query.\r\nlet myvalues = dynamic([2, 6, 1, 9, 2, 5, 7, 1, 0, 9]);\r\nprint series_stats(myvalues);",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `series_stats_dynamic()` function in KQL\r\n\r\n### Purpose\r\nThe `series_stats_dynamic()` function is used to **compute summary statistics for a dynamic array column across all rows**, returning a dynamic object with the statistics for each row.  \r\nThis function is especially useful for analyzing and summarizing time series data stored as dynamic arrays in a column.  \r\n\r\n### Behavior\r\n`series_stats_dynamic()` computes a set of descriptive statistics for each dynamic array, including count, min, max, average (mean), variance, standard deviation, and sum.  \r\nThe results are returned as a dynamic object, allowing you to easily access individual metrics using dot notation.  \r\n\r\n### Syntax\r\n\r\n```kql\r\nseries_stats_dynamic(series)\r\n```  \r\n\r\n### Performance notes\r\n`series_stats_dynamic()` operates efficiently row by row on dynamic array columns without needing to expand or flatten the arrays.  \r\nIt is optimized for large time series scenarios where each entity (e.g., device, customer, sensor) has its own array of measurements.  \r\n\r\n### Remarks\r\n- The output dynamic object can be accessed using dot notation (e.g., `stats.mean`, `stats.stdev`).  \r\n- Unlike `series_stats()`, which can be used inside `project` to compute multiple statistics at once, `series_stats_dynamic()` is designed for dynamic array columns directly and is more concise when working with dynamic objects.  \r\n- Helps reduce the need for multiple separate expressions to calculate different summary metrics on the same array.  \r\n- If the input array is empty or null, all returned statistics will also be null.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This script demonstrates how to create a dynamic array and analyze it using the `series_stats_dynamic()` function.\r\n//\r\n// 1️⃣ It defines a dynamic array named 'myvalues' using the `dynamic()` function.\r\n//     The array contains the numeric values: [2, 6, 1, 9, 2, 5, 7, 1, 0, 9].\r\n//\r\n// 2️⃣ The `series_stats_dynamic()` function is then applied to this array to compute a comprehensive set of summary statistics, including:\r\n//     - Count\r\n//     - Minimum\r\n//     - Maximum\r\n//     - Average (mean)\r\n//     - Variance\r\n//     - Standard deviation\r\n//     - Sum\r\n//\r\n// 3️⃣ The results are displayed using `print`, which outputs the statistics as a dynamic object,\r\n//     allowing for easy inspection and detailed analysis of the array's properties in a single view.\r\n//\r\n// This approach is useful for quickly gaining insights into the distribution and characteristics of numeric series data directly in KQL.\r\nlet myvalues = dynamic([2, 6, 1, 9, 2, 5, 7, 1, 0, 9]);\r\nprint series_stats_dynamic(myvalues);",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes storm event data for the year 2007 by performing the following steps:\r\n//\r\n// 1️⃣ It uses the `make-series` operator to calculate the daily average property damage ('avg(DamageProperty)').\r\n//     - The time axis is defined from January 1, 2007, to December 31, 2007, with a 1-day bin size.\r\n//     - Results are grouped by 'EventType', so each event type produces its own time series of daily average damage values.\r\n//\r\n// 2️⃣ The `series_stats_dynamic()` function is applied to each time series ('avg_DamageProperty') to compute detailed summary statistics, including:\r\n//     - Minimum (min)\r\n//     - Maximum (max)\r\n//     - Average (avg)\r\n//     - Standard deviation (stdev)\r\n//\r\n// 3️⃣ The `project` operator is used to select and display:\r\n//     - 'EventType'\r\n//     - The computed statistics (min, max, avg, stdev)\r\n//\r\n// This approach provides a concise summary of how daily average property damage values vary across different storm event types over time,\r\n// making it easier to identify trends and compare severity between event categories.\r\ndatabase(\"Samples\").StormEvents\r\n| make-series avg(DamageProperty) on StartTime in range (todatetime(\"20070101\"), todatetime(\"20071231\"), 1d) by EventType\r\n| extend x = series_stats_dynamic(avg_DamageProperty)\r\n| project EventType, x.min, x.max, x.avg, x.stdev;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `strcat()` function in KQL\r\n\r\n### Purpose\r\nThe `strcat()` function is used to **concatenate two or more string expressions into a single string value**.  \r\nIt is one of the most common functions for building dynamic text or formatted output columns.\r\n\r\n### Behavior\r\n`strcat()` accepts multiple string arguments and joins them together in the order provided, without any additional separator.  \r\nIf any argument is null, it is treated as an empty string during concatenation.\r\n\r\n### Syntax\r\n\r\n```kql\r\nstrcat(string1, string2, ..., stringN)\r\n```  \r\n\r\n### Performance notes\r\n`strcat()` is evaluated row by row and is highly efficient even when used on large datasets.  \r\nIt is commonly combined with other functions (e.g., `tostring()`, `format_datetime()`) to build complex string representations.\r\n\r\n### Remarks\r\n- Arguments that are not explicitly strings (such as integers or datetimes) need to be converted using `tostring()` before concatenation.  \r\n- When you need to include delimiters or separators (e.g., commas, spaces), explicitly include them as arguments (e.g., `strcat(\"A\", \", \", \"B\")`).  \r\n- Useful for generating readable labels, keys, or composite identifiers in dashboards and reports.  \r\n- `strcat()` returns an empty string if all input arguments are null.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves data from the 'Products' table in the 'ContosoSales' database.\r\n//\r\n// 1️⃣ The `project` operator selects three columns to include in the output:\r\n//     - 'ProductName'\r\n//     - 'ProductCategoryName'\r\n//     - 'ProductSubcategoryName'\r\n//\r\n// 2️⃣ The `extend` operator creates a new column named 'CategoryLongText'.\r\n//     - This column is generated using the `strcat()` function, which concatenates 'ProductCategoryName',\r\n//       a literal period (\".\") as a separator, and 'ProductSubcategoryName'.\r\n//     - The result is a combined descriptive string such as \"Accessories.Bags\".\r\n//\r\n// 3️⃣ The `take 10` operator limits the output to the first 10 rows.\r\n//\r\n// This approach is helpful for creating composite category labels, making it easier to display or analyze hierarchical product information.\r\ndatabase('ContosoSales').Products\r\n| project ProductName, ProductCategoryName, ProductSubcategoryName\r\n| extend CategoryLongText = strcat(ProductCategoryName,\".\",ProductSubcategoryName)\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `split()` function in KQL\r\n\r\n### Purpose\r\nThe `split()` function is used to **divide a string into an array of substrings**, using a specified delimiter.  \r\nThis is particularly useful when dealing with delimited text data or when needing to extract specific parts of a string.\r\n\r\n### Behavior\r\n`split()` takes a string and splits it at each occurrence of the specified delimiter, returning a dynamic array of string segments.  \r\nOptionally, you can specify a limit on the maximum number of substrings to return.\r\n\r\n### Syntax\r\n\r\n```kql\r\nsplit(source, delimiter [, requestedCount])\r\n```  \r\n- `source`: The string expression to split.  \r\n- `delimiter`: The string delimiter to split by.  \r\n- `requestedCount`: (Optional) The maximum number of substrings to include in the result.  \r\n\r\n### Performance notes\r\n`split()` is evaluated row by row and is efficient for processing strings in large datasets.  \r\nIt works well in combination with dynamic indexing or `mv-apply` for further processing of resulting arrays.\r\n\r\n### Remarks\r\n- The returned value is a dynamic array, so elements can be accessed using array notation (e.g., `split(Column, \".\")[0]`).  \r\n- If `requestedCount` is specified, the last element in the array will contain the remainder of the string.  \r\n- Useful for parsing hierarchical keys, composite identifiers, or extracting components from structured strings.  \r\n- If the delimiter is not found in the string, the entire string is returned as a single-element array.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the top 100 records from the 'FileCreationEvents' table in the 'SecurityLogs' database.\r\n//\r\n// 1️⃣ The `project` operator selects only the 'path' column.\r\n//\r\n// 2️⃣ The first `extend` creates a new column 'elementsOfPathArray':\r\n//     - It uses the `split()` function to split the 'path' string by backslashes (\"\\\\\").\r\n//     - The result is a dynamic array containing each part of the path.\r\n//\r\n// 3️⃣ The second `extend` creates a new column 'drive':\r\n//     - It accesses the first element (index 0) of the split array, representing the drive or top-level element.\r\n//\r\n// 4️⃣ The third `extend` creates a new column 'rootFolder':\r\n//     - It accesses the second element (index 1) of the split array, representing the root folder.\r\n//\r\n// 5️⃣ The `take 100` operator limits the result set to the first 100 rows.\r\n//\r\n// ✅ This approach is helpful for breaking down file paths into meaningful components,\r\n// making it easier to analyze directory structures or categorize events based on path hierarchy.\r\ndatabase('SecurityLogs').FileCreationEvents\r\n| project path\r\n| extend elementsOfPathArray = split(path, \"\\\\\")\r\n| extend drive = elementsOfPathArray[0]\r\n| extend rootFolder = elementsOfPathArray[1]\r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves the top 100 records from the 'FileCreationEvents' table in the 'SecurityLogs' database.\r\n//\r\n// 1️⃣ The `project` operator selects the 'path' column from the dataset.\r\n//\r\n// 2️⃣ The first `extend` creates a new column 'elementsOfPathArray':\r\n//     - Uses the `split()` function to split the 'path' string by backslashes (\"\\\\\").\r\n//     - The result is a dynamic array, where each element corresponds to a part of the file path (e.g., drive letter, folders, file name).\r\n//\r\n// 3️⃣ The second `extend` creates a new column 'drive':\r\n//     - Extracts the first element (index 0) of the split array, representing the drive (e.g., \"C:\").\r\n//\r\n// 4️⃣ The third `extend` creates a new column 'rootFolder':\r\n//     - Extracts the second element (index 1) of the split array, representing the root folder.\r\n//\r\n// ✅ Note: When you access an element from an array using indexing syntax (e.g., `elementsOfPathArray[0]`),\r\n// the result is no longer an array but a single string value.\r\n//\r\n// 5️⃣ The `take 100` operator limits the result set to the first 100 rows.\r\n//\r\n// This approach is useful for breaking down and analyzing hierarchical file paths,\r\n// allowing you to easily examine or categorize parts of the path such as drives and top-level folders.\r\ndatabase('SecurityLogs').FileCreationEvents\r\n| project path\r\n| extend elementsOfPathArray = split(path, \"\\\\\")\r\n| extend drive = elementsOfPathArray[0]\r\n| extend rootFolder = elementsOfPathArray[1]\r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `extract()` function in KQL\r\n\r\n### Purpose\r\nThe `extract()` function is used to **extract a substring from a string using a regular expression pattern**.  \r\nIt is useful for parsing and retrieving specific portions of text data, such as IDs, codes, or segments that match a defined pattern.\r\n\r\n### Behavior\r\n`extract()` takes a regex pattern and retrieves the first capturing group that matches the pattern within the source string.  \r\nIf no match is found, it returns null.\r\n\r\n### Syntax\r\n\r\n```kql\r\nextract(regex, captureGroup, text)\r\n```  \r\n- `regex`: The regular expression pattern to apply.  \r\n- `captureGroup`: The 1-based index of the capturing group to extract.  \r\n- `text`: The string expression from which to extract the substring.  \r\n\r\n### Performance notes\r\n`extract()` is evaluated row by row and is optimized for handling large text columns efficiently.  \r\nProperly designed regular expressions are crucial for performance and correctness.\r\n\r\n### Remarks\r\n- Only the specified capturing group is returned, not the full match.  \r\n- If the pattern does not match, the result will be null for that row.  \r\n- Useful for cleaning or restructuring semi-structured text data, such as logs, file paths, or JSON fragments.  \r\n- You can use standard regex constructs, including anchors, quantifiers, character classes, and groups.  \r\n- When parsing structured fields from strings, consider performance impacts of complex regex patterns.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves data from the 'FileCreationEvents' table in the 'SecurityLogs' database,\r\n// focusing on analyzing file paths.\r\n//\r\n// 1️⃣ The `project` operator selects the 'path' column for further analysis.\r\n//\r\n// 2️⃣ The `extend` operator creates a new column named 'drive':\r\n//     - It uses the `extract()` function with the regular expression pattern \"[A-Z]:\", which matches a single uppercase letter\r\n//       followed by a colon (e.g., \"C:\", \"D:\") — typical drive letters in Windows paths.\r\n//     - The second argument (0) indicates that we want to return the entire match (i.e., group 0, the whole match).\r\n//     - The 'path' column is used as the source string from which to extract the drive letter.\r\n//\r\n// 3️⃣ The `take 100` operator limits the output to the first 100 rows.\r\n//\r\n// ✅ This approach is helpful for quickly identifying which drive each file path is associated with,\r\n// allowing for further categorization or security analysis.\r\ndatabase('SecurityLogs').FileCreationEvents\r\n| project path\r\n| extend drive = extract(\"[A-Z]:\", 0, path) \r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query retrieves data from the 'Partner' table in the 'FindMyPartner' database,\r\n// focusing on parsing and grouping information from the 'Contact' field.\r\n//\r\n// 1️⃣ The `extend` operator creates a new column called 'ContactName':\r\n//     - It uses the `tolower()` function to convert the 'Contact' field value to lowercase to ensure uniformity.\r\n//     - The `extract()` function is then used with the regular expression @\"^(.*\\\\)?([^@]*)(@.*)?$\":\r\n//         - This regex breaks down possible user identifiers that might contain a domain prefix or email domain.\r\n//         - The second capture group `([^@]*)` extracts the main username part, ignoring any domain or prefix information.\r\n//         - Example matches:\r\n//             - \"DOMAIN\\\\user\" ➜ \"user\"\r\n//             - \"user@domain.com\" ➜ \"user\"\r\n//             - \"user\" ➜ \"user\"\r\n//\r\n// 2️⃣ The `summarize` operator groups the results by the extracted 'ContactName' and the original 'Contact' field.\r\n//     - For each group, it counts the number of records (using `count()`).\r\n//\r\n// ✅ This approach is useful for consolidating and analyzing partner contact data regardless of domain or prefix variations,\r\n// allowing you to identify unique usernames and their occurrences, even if stored with different formats.\r\ndatabase('FindMyPartner').Partner\r\n| extend ContactName = extract(@\"^(.*\\\\)?([^@]*)(@.*)?$\", 2, tolower(Contact))\r\n| summarize count() by ContactName, Contact;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `parse_json()` function in KQL\r\n\r\n### Purpose\r\nThe `parse_json()` function is used to **convert a JSON-formatted string into a dynamic object**.  \r\nThis allows you to access nested fields and work with JSON data stored in string columns as structured data.\r\n\r\n### Behavior\r\nWhen you pass a valid JSON string to `parse_json()`, it returns a dynamic object (similar to a hierarchical dictionary or array).  \r\nYou can then use dot notation or bracket notation to reference specific fields or elements within the parsed JSON.\r\n\r\n### Syntax\r\n\r\n```kql\r\nparse_json(jsonString)\r\n```  \r\n- `jsonString`: A string expression that contains valid JSON text.\r\n\r\n### Performance notes\r\n`parse_json()` is evaluated row by row and is efficient if the JSON data is well-formed and reasonably sized.  \r\nProcessing very large JSON strings can impact query performance, so consider pre-processing or filtering when possible.\r\n\r\n### Remarks\r\n- If the string is not valid JSON, the function returns null.  \r\n- Once parsed, fields can be accessed using dynamic object syntax: `myObject.fieldName` or `myObject[\"fieldName\"]`.  \r\n- Supports nested objects and arrays, allowing you to extract deeply nested properties.  \r\n- Often used together with `mv-expand` or `bag_unpack` to further flatten or expand JSON structures.  \r\n- Useful when dealing with logs, telemetry, or custom columns storing JSON payloads.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes data from the 'RawSysLogs' table in the 'SampleLogs' database,\r\n// focusing on extracting specific values from JSON-formatted fields.\r\n//\r\n// 1️⃣ The `project` operator selects the 'fields' column, which contains JSON-formatted text.\r\n//\r\n// 2️⃣ The first `extend` operator creates a new column 'parsed_fields':\r\n//     - It uses the `parse_json()` function to convert the JSON string in 'fields' into a dynamic object.\r\n//     - This allows you to easily access individual fields inside the JSON structure.\r\n//\r\n// 3️⃣ The second `extend` operator creates two new columns:\r\n//     - 'current_tasks_count': Extracted from 'parsed_fields[\"current_tasks_count\"]'.\r\n//     - 'current_workers_count': Extracted from 'parsed_fields.current_workers_count' (dot notation works as well).\r\n//\r\n// 4️⃣ The `project` operator selects and displays the original 'fields' column alongside the newly extracted fields.\r\n//\r\n// 5️⃣ The `take 10` operator limits the result to the first 10 rows.\r\n//\r\n// ✅ This approach is useful for breaking out and analyzing specific metrics stored in JSON logs,\r\n// allowing you to work with them as separate columns without losing the original raw data.\r\ndatabase('SampleLogs').RawSysLogs\r\n| project fields\r\n| extend parsed_fields = parse_json(fields)\r\n| extend\r\n    current_tasks_count = parsed_fields[\"current_tasks_count\"],\r\n    current_workers_count = parsed_fields.current_workers_count\r\n| project fields, current_tasks_count, current_workers_count\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `parse` operator in KQL\r\n\r\n### Purpose\r\nThe `parse` operator is used to **extract structured fields from unstructured text columns using pattern-based expressions**.  \r\nIt is especially useful when dealing with logs or free-text data where key details need to be extracted into separate columns.\r\n\r\n### Behavior\r\n`parse` matches a specified expression pattern against each row's input text column and extracts named fields directly into new columns.  \r\nThe pattern syntax supports placeholders (e.g., `%s`, `%d`) and named column assignments.  \r\n\r\n### Syntax\r\n\r\n```kql\r\n| parse [kind=regex|simple] [columnName] with [pattern]\r\n```  \r\n- `columnName`: The column containing the text to parse.  \r\n- `pattern`: The pattern expression defining the expected structure, using placeholders to capture parts of the text.  \r\n- `kind`: Optional; specifies whether to use `simple` (default) or `regex` mode.\r\n\r\n### Performance notes\r\nThe `parse` operator is efficient for extracting fields when the log or text data structure is predictable.  \r\nUsing simpler patterns or explicit placeholders (instead of full regex) improves performance.\r\n\r\n### Remarks\r\n- In `simple` mode (default), the pattern uses placeholders similar to printf formatting: `%s` for string, `%d` for integer, `%t` for datetime, `%f` for floating point, etc.  \r\n- In `regex` mode, full regular expressions are supported, providing more flexibility but potentially higher processing cost.  \r\n- If the input text does not match the pattern, the extracted fields will be null for that row.  \r\n- Useful for extracting structured properties like IP addresses, timestamps, or IDs directly from log messages.  \r\n- Supports parsing directly into multiple new columns in a single operator call, which simplifies query logic and improves readability.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes the 'OutboundBrowsing' table in the 'SecurityLogs' database,\r\n// focusing on parsing user agent strings.\r\n//\r\n// 1️⃣ The `project` operator selects only the 'user_agent' column to include in the output.\r\n//\r\n// 2️⃣ The `parse` operator is used to extract part of the user agent string:\r\n//     - The pattern specified is \"Mozilla/\" followed by a placeholder `parsed_user_agent`.\r\n//     - This means the query will look for strings that start with \"Mozilla/\" and capture everything that follows into the new column `parsed_user_agent`.\r\n//     - If a row's 'user_agent' value does not match this pattern, `parsed_user_agent` will be set to null.\r\n//\r\n// 3️⃣ The `take 10` operator limits the results to the first 10 rows.\r\n//\r\n// ✅ This approach is helpful when you want to isolate or analyze the portion of the user agent string that comes after \"Mozilla/\",\r\n// for example, to classify browsers or devices based on remaining signature text.\r\ndatabase('SecurityLogs').OutboundBrowsing\r\n| project user_agent\r\n| parse user_agent with \"Mozilla/\" parsed_user_agent\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes the 'OutboundBrowsing' table in the 'SecurityLogs' database,\r\n// focusing on extracting parts of user agent strings.\r\n//\r\n// 1️⃣ The `project` operator selects the 'user_agent' column for further analysis.\r\n//\r\n// 2️⃣ The `parse` operator is used to attempt to extract the portion of the string that starts after \"Chrome/\":\r\n//     - The pattern \"Chrome/\" followed by the placeholder `parsed_user_agent` instructs KQL to capture everything that comes after \"Chrome/\" in each 'user_agent' string.\r\n//     - If a user agent string does not contain \"Chrome/\", the `parsed_user_agent` field will be null for that row.\r\n//\r\n// 3️⃣ The `take 10` operator limits the output to the first 10 rows.\r\n//\r\n// ✅ In this specific dataset, no 'user_agent' values start with \"Chrome/\", so the `parsed_user_agent` column will be empty (null) in the results.\r\n//\r\n// This pattern is helpful for isolating browser version information or other trailing details when \"Chrome/\" appears in user agent strings.\r\ndatabase('SecurityLogs').OutboundBrowsing\r\n| project user_agent\r\n| parse user_agent with \"Chrome/\" parsed_user_agent\r\n| take 10;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query analyzes user agent strings in the 'OutboundBrowsing' table from the 'SecurityLogs' database,\r\n// focusing on extracting and filtering specific components.\r\n//\r\n// 1️⃣ The `project` operator selects the 'user_agent' column for further analysis.\r\n//\r\n// 2️⃣ The `parse` operator extracts key parts of the user agent string using a structured pattern:\r\n//     - The pattern matches any text before \"Mozilla/\" (using `*`), then extracts `mozilla_version` after \"Mozilla/\".\r\n//     - After that, it expects an opening parenthesis \"(\" and captures `windows_version` up to the closing parenthesis \")\".\r\n//     - Finally, it captures any remaining text into a column named `remaining` (even though it isn't projected later).\r\n//     - Example user agent: \"SomeText Mozilla/5.0 (Windows NT 10.0; Win64; x64) MoreStuff\"\r\n//       → `mozilla_version`: \"5.0\"\r\n//       → `windows_version`: \"Windows NT 10.0; Win64; x64\"\r\n//\r\n// 3️⃣ The second `project` keeps only 'user_agent', 'mozilla_version', and 'windows_version' columns.\r\n//\r\n// 4️⃣ The `where` clause filters out rows where 'windows_version' contains a semicolon (`;`).\r\n//     - This can be useful to focus on simpler or specific user agent strings without detailed subcomponents.\r\n//\r\n// 5️⃣ The `take 100` operator limits the output to the first 100 rows.\r\n//\r\n// ✅ This query is useful for isolating and analyzing browser version and Windows platform information in user agent data,\r\n// while filtering out detailed or compound entries for a cleaner view.\r\ndatabase('SecurityLogs').OutboundBrowsing\r\n| project user_agent\r\n| parse user_agent with * \"Mozilla/\" mozilla_version \"(\" windows_version \")\" remaining\r\n| project user_agent, mozilla_version, windows_version\r\n| where windows_version !contains \";\"\r\n| take 100;",
            "outputs": []
        },
        {
            "kind": "markdown",
            "source": "## `datatable` operator in KQL\r\n\r\n### Purpose\r\nThe `datatable` operator is used to **create an inline, in-memory table directly within a query**.  \r\nThis is especially useful for defining small test datasets, temporary lookup tables, or providing static lists for joins and filters.  \r\n\r\n### Behavior\r\n`datatable` defines a table schema explicitly, including column names and data types, followed by rows of data specified in brackets.  \r\nThe resulting table behaves like any other table in KQL and can be joined, projected, or filtered in subsequent query steps.  \r\n\r\n### Syntax\r\n\r\n```kql\r\ndatatable (ColumnName1: type1, ColumnName2: type2, ...) [ row1, row2, ... ]\r\n```  \r\n\r\n### Performance notes\r\nSince `datatable` is evaluated entirely in memory and typically holds only small data, it is very fast and does not involve scanning persisted data.  \r\nIdeal for scenarios where small static datasets are needed without creating permanent tables.  \r\n\r\n### Remarks\r\n- Supports all standard KQL data types (e.g., int, long, real, string, datetime, guid).  \r\n- The `datatable` content is static and cannot be dynamically changed during execution.  \r\n- Useful for scenarios like creating filter keys for joins, demonstrating query logic, or mocking data for presentations and testing.  \r\n- Improves readability and simplifies maintenance by embedding example data directly within the query script.  ",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This datatable is used to demonstrate a simple dataset creation directly within a KQL query.\r\n//\r\n// It defines a temporary in-memory table with two columns:\r\n//   - 'ID' of type int (integer).\r\n//   - 'Name' of type string.\r\n//\r\n// The dataset includes three sample rows:\r\n//   - (1, \"Name-01\")\r\n//   - (2, \"Name-02\")\r\n//   - (3, \"Name-03\")\r\n//\r\n// This is useful for testing, demonstrating functions, or quickly prototyping queries without relying on existing tables.\r\ndatatable (ID: int, Name: string)\r\n[\r\n    1, \"Name-01\",\r\n    2, \"Name-02\",\r\n    3, \"Name-03\"\r\n];",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query creates a temporary in-memory table named 'dtTable' using 'datatable'.\r\n// The table has two columns:\r\n//   - 'ID' of type int.\r\n//   - 'Name' of type string.\r\n//\r\n// It includes three sample rows:\r\n//   - (1, \"Name-01\")\r\n//   - (2, \"Name-02\")\r\n//   - (3, \"Name-03\")\r\n//\r\n// After defining the table, the query filters it using 'where' to return only the row where 'ID' equals 1.\r\n//\r\n// This technique is useful for testing or illustrating query logic with sample data before applying it to real tables.\r\nlet dtTable = datatable (ID: int, Name: string)\r\n[\r\n    1, \"Name-01\",\r\n    2, \"Name-02\",\r\n    3, \"Name-03\"\r\n];\r\ndtTable \r\n| where ID == 1;",
            "outputs": []
        },
        {
            "kind": "code",
            "source": "// This query demonstrates how to filter sales records by specific product keys.\r\n//\r\n// 1️⃣ A temporary in-memory datatable named 'dtProductKeys' is created using 'datatable'.\r\n//     - It has a single column 'ProductKey' of type int.\r\n//     - It includes two sample product keys: 261 and 2402.\r\n//\r\n// 2️⃣ Sales data is retrieved from the 'SalesFact' table in the 'ContosoSales' database.\r\n//     - Only the columns 'SalesAmount' and 'ProductKey' are projected.\r\n//\r\n// 3️⃣ An inner join is performed between the sales data and 'dtProductKeys'.\r\n//     - This filters the sales records to include only those matching the specified product keys.\r\n//\r\n// 4️⃣ The resulting columns are then projected using custom aliases for clearer, descriptive output:\r\n//     - 'ContosoSales.SalesFact' for 'SalesAmount'.\r\n//     - 'ContosoSales.ProductKey' for 'ProductKey'.\r\n//\r\n// This pattern is useful for applying dynamic filters from temporary lists (e.g., user-selected IDs or keys).\r\nlet dtProductKeys = datatable (ProductKey: int)\r\n[\r\n    261,\r\n    2402\r\n];\r\ndatabase(\"ContosoSales\").SalesFact\r\n| project SalesAmount, ProductKey\r\n| join kind=inner dtProductKeys on $left.ProductKey == $right.ProductKey\r\n| project \r\n    [\"ContosoSales.SalesFact\"] = SalesAmount,\r\n    [\"ContosoSales.ProductKey\"] = ProductKey;",
            "outputs": []
        }
    ],
    "metadata": {
        "connection": {
            "cluster": "https://help.kusto.windows.net",
            "database": "Samples"
        }
    }
}